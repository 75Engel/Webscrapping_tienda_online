{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de datos a través de *Beautiful Soap 4*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Obtención de datos general de los datos.\n",
    "\n",
    "En un primer paso vamos a hacer llamadas a través de la librería de BeautifulSoap4 para obtener datos generales desde la primera pagina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre lo primero es importar las librerías "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data_science\\\\Javier\\\\Repositorios\\\\Proyecto_tienda_online'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(os.path.split(os.getcwd())[0])\n",
    "folder=os.getcwd()\n",
    "folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conectamos la pagina web para acceder a ella y *arañar* los datos de su página web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.amantis.net/productos-amantis/\"              # lista productos\n",
    "url_principal=\"https://www.amantis.net/\"                        # productos\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = response.content\n",
    "soup = bs(html, \"lxml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.- Vamos a obtener información de los datos existente en la URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_h3 = soup.find_all(\"h3\")\n",
    "all_h3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.- Extraemos los nombres desde esta URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "['LIZO 2', 'FOXTAIL', 'TOBOGANE HOT RABBIT', 'Vibrador Líquido con sabor Desliz! VIBRAGEL 30ml', 'MENEO sube y baja', 'TOBOGANE', 'CRISTALINO XL', 'FRESH GIRL', 'BALLENATO', 'Bacanal Gel Anal monodosis', 'FLOGGY - Flogger BDSM de piel vegana', 'CAMILLE', 'SAZZIA', 'AVATAR FIRST LOVER', 'AVENTURE -  Vibrador con imán y mando a distancia', 'REGGIA', 'MAGIC CUP', 'BUTTERFLY', 'EXPLORE PERFECT', 'Vibrador Líquido con sabor Desliz! VIBRAGEL 10ml', 'EISSELY', 'TRIS-TRAS', 'TOK anal 10', 'TROMPI']\n"
     ]
    }
   ],
   "source": [
    "titulos=soup.find_all(\"h3\")\n",
    "name=[]\n",
    "for titulo in titulos[1:]:\n",
    "    nombre=titulo.get_text(strip=True).split(',')[0]\n",
    "    name.append(nombre)\n",
    "print(len(name))\n",
    "print(name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.- Con este codigo extraigo la descripción del producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "[['Dildo de suave silicona en 3 tamaños'], ['plug anal cola de zorro de 35cm'], ['el superventas de amantis ¡mejorado!'], ['hormigueo...'], ['placer realista con control remoto'], ['el vibrador doble más vendido'], ['gran dildo transparente de 22cm'], ['6 Kilos y 40cm de piel real disfrutable'], ['tu vibrador a distancia con aleta móvil y sumergible...'], ['Pack de 10 uds'], ['No hay datos'], ['body camiseta floral con cuello halter de amantis'], ['masturbador hiperrealista'], ['amante realista de suave silicona'], ['No hay datos'], ['masturbador masculino doble'], ['Masturbarse mola un huevo'], ['Bodysocking de fina malla con detalle floral'], ['posiblemente el dildo realista perfecto'], ['hormigueo...'], ['medias abiertas para recorrer'], ['anilla con vibrador para doble penetración TRINITY...'], ['Vibrador anal con 10 modos de vibración'], ['vibrador sumergible ideal para Punto-G']]\n"
     ]
    }
   ],
   "source": [
    "titulos=soup.find_all(\"h3\")\n",
    "desc=[]\n",
    "\n",
    "for titulo in titulos[1:]:\n",
    "    description=titulo.get_text(strip=True).split(', ')[1:]\n",
    "    if description==[]:\n",
    "        description=[\"No hay datos\"]\n",
    "    desc.append(description)\n",
    "print(len(desc))\n",
    "print(desc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en algunos casos no hay información. \n",
    "\n",
    "En otros casos la separación de la descripción, no se ha separado.\n",
    "\n",
    "Por este motivo, haremos la extracción unicamente de todo y posteriormente lo trataremos con la librería de *pandas*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.- Con esto extraigo los links de los productos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "productos = soup.find_all(class_='caption')\n",
    "lista_URLs = []\n",
    "for producto in productos[8:]:\n",
    "    URL_producto = producto.find('a')['href']\n",
    "    lista_URLs.append(URL_producto)\n",
    "\n",
    "len(lista_URLs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta información es importante ya que nos permitirá obtener las direcciones de cada producto para hacer posteriormente la información desde cada producto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e.- Obtenemos la información de precio rebajado de cada producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_price=soup.find_all(\"span\",class_=\"productSpecialPrice\")\n",
    "price=[]\n",
    "\n",
    "for precio in all_price:\n",
    "    item_price=precio.get_text(strip=True).replace(\",\", \".\").split('€')[0]\n",
    "    price.append(item_price)\n",
    "len(price)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Este es el codigo completo para extraer la información de cada página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Data_science\\Javier\\Repositorios\\Proyecto_tienda_online\\Notebook\\Proyecto_scrap.ipynb Cell 21\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data_science/Javier/Repositorios/Proyecto_tienda_online/Notebook/Proyecto_scrap.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     URL_producto \u001b[39m=\u001b[39m producto\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data_science/Javier/Repositorios/Proyecto_tienda_online/Notebook/Proyecto_scrap.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     lista_URLs\u001b[39m.\u001b[39mappend(URL_producto)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Data_science/Javier/Repositorios/Proyecto_tienda_online/Notebook/Proyecto_scrap.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m df_productos \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m\"\u001b[39m\u001b[39mName\u001b[39m\u001b[39m\"\u001b[39m: name,\u001b[39m\"\u001b[39m\u001b[39mDescription\u001b[39m\u001b[39m\"\u001b[39m: desc,\u001b[39m\"\u001b[39m\u001b[39mPrice\u001b[39m\u001b[39m\"\u001b[39m:price,\u001b[39m\"\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m\"\u001b[39m:lista_URLs})\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data_science/Javier/Repositorios/Proyecto_tienda_online/Notebook/Proyecto_scrap.ipynb#X26sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m df_productos\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:663\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    657\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    658\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    659\u001b[0m     )\n\u001b[0;32m    661\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    662\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 663\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    664\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    665\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:494\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    492\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 494\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:119\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    117\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    120\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    665\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[0;32m    666\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 667\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    669\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[0;32m    670\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    671\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    672\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "titulos=soup.find_all(\"h3\")\n",
    "name=[]\n",
    "desc=[]\n",
    "price=[]\n",
    "lista_URLs = []\n",
    "\n",
    "for titulo in titulos:\n",
    "    nombre=titulo.get_text(strip=True).split(',')[0]\n",
    "    name.append(nombre)\n",
    "    description=titulo.get_text(strip=True).split(', ')[1:]\n",
    "    if description==[]:\n",
    "        description=[\"No hay datos\"]\n",
    "    desc.append(description)\n",
    "\n",
    "all_price=soup.find_all(\"span\",class_=\"productSpecialPrice\")\n",
    "\n",
    "for precio in all_price:\n",
    "    item_price=precio.get_text(strip=True).replace(\",\", \".\").split('€')[0]\n",
    "    price.append(item_price)\n",
    "\n",
    "productos = soup.find_all(class_='caption')\n",
    "for producto in productos[8:]:\n",
    "    URL_producto = producto.find('a')['href']\n",
    "    lista_URLs.append(URL_producto)\n",
    "\n",
    "df_productos = pd.DataFrame({\"Name\": name,\"Description\": desc,\"Price\":price,\"link\":lista_URLs})\n",
    "df_productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Top ventas en amantis'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.amantis.net/lizo-2-dildo-suave-silicona-3-tamanos/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_URLs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a sacar la información de todas las páginas posibles.\n",
    "\n",
    "Para esto vamos a obtener las URLs, como en el punto d. \n",
    "\n",
    "Para extraer los links de los productos y ver si podemos sacar la información de su página concreta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.amantis.net/lizo-2-dildo-suave-silicona-3-tamanos/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_URLs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un nuevo request para el primer libro: \n",
    "r = requests.get(lista_URLs[0])\n",
    "\n",
    "# Creamos una sopa específica con la info de cada libro\n",
    "soup_producto = bs(r.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIZO 2, Dildo de suave silicona en 3 tamaños\n"
     ]
    }
   ],
   "source": [
    "name = soup_producto.find('h1').text\n",
    "print(name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.- Vamos a entrar en una página general para extraer información de ella.\n",
    "\n",
    "Principalmente vamos a ver como podemos, con un bucle, sacar todos los datos de cada producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=3\n",
    "url = \"https://www.amantis.net/productos-amantis/\"              # lista productos\n",
    "URL = url+'page' + str(page)+'/'\n",
    "response = requests.get(URL)\n",
    "titulos=soup.find_all(\"h3\")\n",
    "titulos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el bucle para obtener información anterior de las 5 primeras paginas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagina 2\n",
      "Pagina 3\n",
      "Pagina 4\n",
      "Nombres:  72\n",
      "Descripción:  72\n",
      "URL:  72\n",
      "Precios:  69\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.amantis.net/productos-amantis/\"              # lista productos\n",
    "url_principal=\"https://www.amantis.net/\"                        # productos\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "pages= np.arange(2,5)                                   # La primera pagina tiene una serie de datos que no debemos de recoger.\n",
    "name=[]\n",
    "desc=[]\n",
    "price=[]\n",
    "lista_URLs = []\n",
    "\n",
    "for page in pages:\n",
    "    if page == 1:\n",
    "        print(\"Pagina\",page)\n",
    "        URL = url\n",
    "        response = requests.get(url)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        titulos=soup.find_all(\"h3\")\n",
    "\n",
    "        for titulo in titulos:\n",
    "            nombre=titulo.get_text(strip=True).split(',')[0]\n",
    "            name.append(nombre)\n",
    "            description=titulo.get_text(strip=True).split(', ')[1:]\n",
    "            if description==[]:\n",
    "                description=[\"No hay datos\"]\n",
    "            desc.append(description)\n",
    "            print(nombre)\n",
    "\n",
    "        all_price=soup.find_all(\"span\",class_=\"productSpecialPrice\")\n",
    "\n",
    "        for precio in all_price:\n",
    "            item_price=precio.get_text(strip=True).replace(\",\", \".\").split('€')[0]\n",
    "            price.append(item_price)\n",
    "\n",
    "\n",
    "        productos = soup.find_all(class_='caption')\n",
    "\n",
    "        for producto in productos[8:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "            print(URL_producto)\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print(\"Pagina\",page)\n",
    "        URL = url+'page' + str(page)+'/'\n",
    "        response = requests.get(URL)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        titulos=soup.find_all(\"h3\")\n",
    "\n",
    "        for titulo in titulos:\n",
    "            nombre=titulo.get_text(strip=True).split(',')[0]\n",
    "            name.append(nombre)\n",
    "            description=titulo.get_text(strip=True).split(', ')[1:]\n",
    "            if description==[]:\n",
    "                description=[\"No hay datos\"]\n",
    "            desc.append(description)\n",
    "            # print(nombre)\n",
    "\n",
    "        for precio in all_price:\n",
    "            item_price=precio.get_text(strip=True).replace(\",\", \".\").split('€')[0]\n",
    "            price.append(item_price)\n",
    "\n",
    "        productos = soup.find_all(class_='caption')\n",
    "\n",
    "        for producto in productos[8:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "            # print(URL_producto)\n",
    "\n",
    "print(\"Nombres: \",len(name))\n",
    "print(\"Descripción: \",len(desc))\n",
    "print(\"URL: \",len(lista_URLs))\n",
    "print(\"Precios: \",len(price))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que vemos que hay incongruencias en la longitud del tamaño de cada lista, vamos a realizar un Dataframe unicamente de los datos con la misma longitud.\n",
    "\n",
    "Esta problemática la solucionaremos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BISOU</td>\n",
       "      <td>[besos por ondas de succión con la mejor vibra...</td>\n",
       "      <td>https://www.amantis.net/bisou-besos-ondas-succ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POWER UP METER - Bomba de succión con manómetro</td>\n",
       "      <td>[No hay datos]</td>\n",
       "      <td>https://www.amantis.net/power-up-meter-bomba-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TOK2 bala vibradora con mando</td>\n",
       "      <td>[más potente y recargable]</td>\n",
       "      <td>https://www.amantis.net/tok2-bala-vibradora-ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAGIC CUP</td>\n",
       "      <td>[Masturbarse mola un huevo]</td>\n",
       "      <td>https://www.amantis.net/magic-cup-6-masturbado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRIPLE RABBIT</td>\n",
       "      <td>[Conejito vibrador con bolas anales]</td>\n",
       "      <td>https://www.amantis.net/triple-rabbit-conejito...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Name  \\\n",
       "0                                            BISOU   \n",
       "1  POWER UP METER - Bomba de succión con manómetro   \n",
       "2                    TOK2 bala vibradora con mando   \n",
       "3                                        MAGIC CUP   \n",
       "4                                    TRIPLE RABBIT   \n",
       "\n",
       "                                         Description  \\\n",
       "0  [besos por ondas de succión con la mejor vibra...   \n",
       "1                                     [No hay datos]   \n",
       "2                         [más potente y recargable]   \n",
       "3                        [Masturbarse mola un huevo]   \n",
       "4               [Conejito vibrador con bolas anales]   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.amantis.net/bisou-besos-ondas-succ...  \n",
       "1  https://www.amantis.net/power-up-meter-bomba-s...  \n",
       "2  https://www.amantis.net/tok2-bala-vibradora-ma...  \n",
       "3  https://www.amantis.net/magic-cup-6-masturbado...  \n",
       "4  https://www.amantis.net/triple-rabbit-conejito...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_productos = pd.DataFrame({\"Name\": name,\"Description\": desc,\"link\":lista_URLs})\n",
    "df_productos.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.- Verifico que he conseguido las URLs de cada producto, esto es importante para obtener la información de cada producto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraer la información de producto, descripción, enlace y precio tomando los datos desde las URLs de cada producto.\n",
    "\n",
    "Queda pendiente extraer información de los ratings y los comentarios para establecer un estudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagina 1\n",
      "Pagina 2\n",
      "Pagina 3\n",
      "Pagina 4\n",
      "Pagina 5\n",
      "Pagina 6\n",
      "Pagina 7\n",
      "Pagina 8\n",
      "Pagina 9\n",
      "Pagina 10\n",
      "Pagina 11\n",
      "Pagina 12\n",
      "Pagina 13\n",
      "Pagina 14\n",
      "Pagina 15\n",
      "Pagina 16\n",
      "Pagina 17\n",
      "Pagina 18\n",
      "Pagina 19\n",
      "Pagina 20\n",
      "Pagina 21\n",
      "Pagina 22\n",
      "Pagina 23\n",
      "Pagina 24\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.amantis.net/productos-amantis/\"              # lista productos\n",
    "url_principal=\"https://www.amantis.net/\"                        # productos\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "pages= np.arange(1, 25)\n",
    "name=[]\n",
    "desc=[]\n",
    "price=[]\n",
    "lista_URLs = []\n",
    "\n",
    "for page in pages:\n",
    "    if page == 1:\n",
    "        print(\"Pagina\",page)\n",
    "        URL = url\n",
    "        response = requests.get(url)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        titulos=soup.find_all(\"h3\")\n",
    "\n",
    "        for titulo in titulos[1:]:\n",
    "            nombre=titulo.get_text(strip=True).split(',')[0]\n",
    "            name.append(nombre)\n",
    "            description=titulo.get_text(strip=True).split(', ')[1:]\n",
    "            if description==[]:\n",
    "                description=[\"No hay datos\"]\n",
    "            desc.append(description)\n",
    "\n",
    "        all_price=soup.find_all(\"span\",class_=\"productSpecialPrice\")\n",
    "\n",
    "        for precio in all_price:\n",
    "            item_price=precio.get_text(strip=True).replace(\",\", \".\").split('€')[0]\n",
    "            price.append(item_price)\n",
    "\n",
    "\n",
    "        productos = soup.find_all(class_='caption')\n",
    "\n",
    "        for producto in productos[8:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print(\"Pagina\",page)\n",
    "        URL = url+'page' + str(page)+'/'\n",
    "        response = requests.get(URL)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        titulos=soup.find_all(\"h3\")\n",
    "\n",
    "        for titulo in titulos:\n",
    "            nombre=titulo.get_text(strip=True).split(',')[0]\n",
    "            name.append(nombre)\n",
    "            description=titulo.get_text(strip=True).split(', ')[1:]\n",
    "            if description==[]:\n",
    "                description=[\"No hay datos\"]\n",
    "            desc.append(description)\n",
    "\n",
    "        for precio in all_price:\n",
    "            item_price=precio.get_text(strip=True).replace(\",\", \".\").split('€')[0]\n",
    "            price.append(item_price)\n",
    "\n",
    "        productos = soup.find_all(class_='caption')\n",
    "\n",
    "        for producto in productos[8:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_productos = pd.DataFrame({\"Name\": name,\"Description\": desc,\"Price\":price,\"link\":lista_URLs})\n",
    "# df_productos.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que cantidad de datos tenemos al recorrer todos las paginas con productos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres: 576\n",
      "descrip: 576\n",
      "precio: 552\n",
      "URL: 576\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombres:\" ,len(name))\n",
    "print(\"descrip:\" ,len(desc))\n",
    "print(\"precio:\" ,len(price))                    #  Se ve que hay un desajuste en el precio al extraer la información de 1 pagina\n",
    "print(\"URL:\" ,len(lista_URLs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres:\n",
      " ['LIZO 2', 'FOXTAIL', 'TOBOGANE HOT RABBIT', 'Vibrador Líquido con sabor Desliz! VIBRAGEL 30ml', 'MENEO sube y baja']\n",
      "descrip:\n",
      " [['Dildo de suave silicona en 3 tamaños'], ['plug anal cola de zorro de 35cm'], ['el superventas de amantis ¡mejorado!'], ['hormigueo...'], ['placer realista con control remoto']]\n",
      "precio:\n",
      " ['17.99', '9.99', '39.99', '9.99', '44.99']\n",
      "URL:\n",
      " ['https://www.amantis.net/lizo-2-dildo-suave-silicona-3-tamanos/', 'https://www.amantis.net/foxtail-plug-anal-cola-zorro/', 'https://www.amantis.net/tobogane-hot-rabbit-el-superventas-amantis-mejorado/', 'https://www.amantis.net/desliz-vibragel-liquido-vibrador-30ml/', 'https://www.amantis.net/meneo-sube-baja-realista-control-remoto/']\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombres:\\n\" ,name[:5])\n",
    "print(\"descrip:\\n\" ,desc[:5])\n",
    "print(\"precio:\\n\" ,price[:5])\n",
    "print(\"URL:\\n\" ,lista_URLs[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.- Extrayendo la información de los comentarios de cada producto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entrar en un producto para obtener la información de los comentarios que hay:\n",
    "- Usuario\n",
    "- Fecha\n",
    "- Comentario\n",
    "- Rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BALLENATO\n"
     ]
    }
   ],
   "source": [
    "prueba=lista_URLs[1]\n",
    "response = requests.get(prueba)\n",
    "soup_prueba = bs(response.text, 'lxml')\n",
    "\n",
    "titulo=soup_prueba.get_text(strip=True).split(',')[0]\n",
    "print(titulo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fechas**\n",
    "\n",
    "Estos datos son en este momento, *string*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['18/febrero/2023',\n",
       " '01/febrero/2023',\n",
       " '18/noviembre/2022',\n",
       " '06/septiembre/2022',\n",
       " '13/julio/2022',\n",
       " '17/junio/2022',\n",
       " '05/noviembre/2021',\n",
       " '06/julio/2021',\n",
       " '24/mayo/2021',\n",
       " '16/mayo/2021',\n",
       " '30/abril/2021',\n",
       " '16/abril/2021',\n",
       " '01/marzo/2021',\n",
       " '09/febrero/2021',\n",
       " '13/enero/2021',\n",
       " '26/noviembre/2020',\n",
       " '04/noviembre/2020',\n",
       " '25/septiembre/2020',\n",
       " '21/septiembre/2020',\n",
       " '14/septiembre/2020']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating=[]\n",
    "all_ratings = soup_prueba.find_all(\"span\", class_=\"date\")  \n",
    "sep_1=(' ')\n",
    "sep_2=(', ')\n",
    "# pattern = re.compile(sep)\n",
    "for ratings in all_ratings:\n",
    "    rating_coment_1=ratings.get_text(strip=True).split(sep_2)[1]\n",
    "    rating_coment_2=ratings.get_text(strip=True).split(sep_2)[0]\n",
    "    rating_coment_3=rating_coment_2.split(sep_1)[1:]\n",
    "    date_1=rating_coment_3[0]+\"/\"+rating_coment_3[1]\n",
    "    date_2=date_1+\"/\"+rating_coment_1                                       # Estamos pendientes de convertir a fechas, teniendo en cuenta\n",
    "    # date_object = datetime.strptime(date_2,'%d%m%Y')                        # que esta en español\n",
    "    # print(type(date_1))\n",
    "    # print(date_1)\n",
    "    rating.append(date_2)\n",
    "    # print(type(date_2))\n",
    "    # print(date_2)\n",
    "\n",
    "print(len(rating))\n",
    "rating"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usuarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Almudena',\n",
       " 'Tomabel',\n",
       " 'andrea',\n",
       " 'Carlos',\n",
       " 'maria',\n",
       " 'Mar',\n",
       " 'Sasha',\n",
       " 'Andrea',\n",
       " 'irene',\n",
       " 'Jose',\n",
       " 'Ines',\n",
       " 'Rafael',\n",
       " 'andrea',\n",
       " 'Alejandro',\n",
       " 'Elisabet',\n",
       " 'Marta',\n",
       " 'Ana',\n",
       " 'Joana',\n",
       " 'Ricardo',\n",
       " 'fernando']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_comments=[]\n",
    "all_user_comments = soup_prueba.find_all(\"span\", class_=\"name-user\")  \n",
    "\n",
    "for user_comment in all_user_comments:\n",
    "    user_comments.append(user_comment.get_text(strip=True))\n",
    "\n",
    "print(len(user_comments))\n",
    "user_comments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fue mi regalo de cumpleaños...el mejor regalo de mi vida!!!! Muy fácil de poner y muy cómodo.ideal para penetración doble vaginal y dobletes.lo recomiendo 100x100!!!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment=[]\n",
    "all_comments = soup_prueba.find_all(\"p\")\n",
    "for formats in all_comments[-len(rating):]:                     # Creo que me he cargado 'rating'\n",
    "    comment.append(formats.get_text(strip=True))\n",
    "\n",
    "print(comment[1])\n",
    "len(comment)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aquí los datos que estamos obteniendo se pasan a una lista por atributo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.- Este es el código bueno para iterar todos los productos extraer la información siguiente:\n",
    "\n",
    "- Nombre\n",
    "- Descripción\n",
    "- Precio sin rebaja (regular_price)\n",
    "- Precio rebajado (new_price)\n",
    "- Información de cada producto\n",
    "- Lista de usuarios que han comentado\n",
    "- Lista de comentarios\n",
    "- Lista de Fecha de comentarios\n",
    "- Lista de Ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagina 1\n",
      "Pagina 2\n",
      "Pagina 3\n",
      "Pagina 4\n",
      "Pagina 5\n",
      "Pagina 6\n",
      "Pagina 7\n",
      "Pagina 8\n",
      "Pagina 9\n",
      "Pagina 10\n",
      "Pagina 11\n",
      "Pagina 12\n",
      "Pagina 13\n",
      "Pagina 14\n",
      "Pagina 15\n",
      "Pagina 16\n",
      "Pagina 17\n",
      "Pagina 18\n",
      "Pagina 19\n",
      "Pagina 20\n",
      "Pagina 21\n",
      "Pagina 22\n",
      "Pagina 23\n",
      "Pagina 24\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.amantis.net/productos-amantis/\"              # lista productos\n",
    "url_principal=\"https://www.amantis.net/\"                        # productos\n",
    "# pages= np.arange(1,5)\n",
    "pages= np.arange(1, 25)\n",
    "\n",
    "'''Listas a generar con la información de los productos'''\n",
    "lista_URLs = []\n",
    "name=[]\n",
    "subname=[]\n",
    "regular_prices=[]\n",
    "new_price=[]\n",
    "# lista_URLs = []\n",
    "info=[]\n",
    "charac=[]\n",
    "user_comments=[]\n",
    "comment=[]\n",
    "date=[]\n",
    "ratings=[]\n",
    "\n",
    "''' Obtenemos las URLs de los productos para entrar luego en sus URLS y extraer la información'''\n",
    "\n",
    "for page in pages:\n",
    "    if page == 1:\n",
    "        print(\"Pagina\",page)\n",
    "        URL = url\n",
    "        response = requests.get(url)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        productos = soup.find_all(class_='caption')\n",
    "        for producto in productos[8:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "        \n",
    "    else:\n",
    "        print(\"Pagina\",page)\n",
    "        URL = url+'page' + str(page)+'/'\n",
    "        response = requests.get(URL)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        productos = soup.find_all(class_='caption')\n",
    "        for producto in productos[8:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "\n",
    "'''Extraemos la información de cada producto existente'''\n",
    "\n",
    "for URL in lista_URLs:\n",
    "    url_product=URL\n",
    "    response_product = requests.get(url_product)\n",
    "    soup_product = bs(response_product.text, 'lxml')\n",
    "    user_comments_product=[]\n",
    "    date_comments_product=[]\n",
    "    comments_product=[]\n",
    "    rating=[]\n",
    "    \n",
    "\n",
    "    titulos=soup_product.find_all(\"h1\",class_=\"h3\")\n",
    "    for titulo in titulos:\n",
    "        nombre=titulo.get_text(strip=True).split(',')[0]\n",
    "        name.append(nombre)\n",
    "        sub_title=titulo.get_text(strip=True).split(', ')[1:]\n",
    "        if sub_title==[]:\n",
    "            sub_title=[\"No hay datos\"]\n",
    "        subname.append(sub_title)\n",
    "\n",
    "    all_price = soup_product.find_all(\"div\", class_=\"productoPrecio pull-right tdd_precio\")                        \n",
    "    for price_container in all_price:                                                                    \n",
    "        try:\n",
    "            special_price = price_container.find(\"span\", class_=\"productSpecialPrice\")\n",
    "            if special_price:\n",
    "                item_price = float(special_price.get_text(strip=True).replace(\",\", \".\").split('€')[0])\n",
    "                new_price.append(item_price)\n",
    "                regular_price = price_container.find(\"del\").get_text(strip=True)\n",
    "                item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                regular_prices.append(item_regular_price)\n",
    "            else:\n",
    "                regular_price = price_container.find(\"span\").get_text(strip=True)\n",
    "                item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                new_price.append(item_regular_price)\n",
    "                regular_prices.append(None)\n",
    "        except:\n",
    "            new_price.append(None)\n",
    "            regular_prices.append(None)\n",
    "\n",
    "    description=soup_product.find(\"div\", class_=\"description\") \n",
    "    information=description.get_text().split('\\n')[1:5]\n",
    "    info.append(information)\n",
    "    characteristic=description.get_text().split('\\n')[5:-3]\n",
    "    charac.append(characteristic)\n",
    "    '''Vamos a obtener los datos de los comentarios de los usuarios'''\n",
    "\n",
    "    all_user_comments = soup_product.find_all(\"span\", class_=\"name-user\") \n",
    "    for user_comment in all_user_comments:\n",
    "        user_comments_product.append(user_comment.get_text(strip=True))\n",
    "    user_comments.append(user_comments_product)\n",
    "\n",
    "    all_dates = soup_product.find_all(\"span\", class_=\"date\")  \n",
    "    for dates in all_dates:\n",
    "        dates_text=dates.get_text(strip=True)\n",
    "        # dates=datetime.strftime(dates, '%dd/%mm/%Y')\n",
    "        date_comments_product.append(dates_text)\n",
    "        # date_object = datetime.strptime(date_comments_product)\n",
    "    date.append(date_comments_product)\n",
    "\n",
    "    all_comments = soup_product.find_all(\"p\")\n",
    "    for formats in all_comments[-len(date_comments_product):]:\n",
    "        comments_product.append(formats.get_text(strip=True))\n",
    "    comment.append(comments_product)\n",
    "\n",
    "    hearts = soup_product.find_all('div', class_= 'box-description')\n",
    "    for heart in hearts:\n",
    "        heart_rating = heart.find_all('span', class_= 'fas fa-heart')\n",
    "        num_hearts = len(heart_rating)\n",
    "        rating.append(num_hearts)\n",
    "    ratings.append(rating)\n",
    "\n",
    "\n",
    "for i, regular_price in enumerate(regular_prices):\n",
    "    if regular_price is None:\n",
    "        regular_prices[i] = new_price[i]\n",
    "\n",
    "\n",
    "# print('-'*20)\n",
    "# print(name)\n",
    "print('-'*20)\n",
    "print(len(name))\n",
    "# print('-'*20)\n",
    "# print(subname)\n",
    "print('-'*20)\n",
    "print(len(subname))\n",
    "print('-'*20)\n",
    "# print(regular_prices)\n",
    "# print('-'*20)\n",
    "print(len(regular_prices))\n",
    "# print('-'*20)\n",
    "# print(new_price)\n",
    "print('-'*20)\n",
    "print(len(new_price))\n",
    "print('-'*20)\n",
    "print(len(lista_URLs))\n",
    "# print('-'*20)\n",
    "# print(lista_URLs)\n",
    "print('-'*20)\n",
    "print(len(info))\n",
    "# print('-'*20)\n",
    "# # print(charac)\n",
    "print('-'*20)\n",
    "print(len(charac))\n",
    "# print('-'*20)\n",
    "# # print(user_comments)\n",
    "print('-'*20)\n",
    "print(len(user_comments))\n",
    "# print('-'*20)\n",
    "# # print(comment)\n",
    "print('-'*20)\n",
    "print(len(comment))\n",
    "# # print('-'*20)\n",
    "# # print(date)\n",
    "print('-'*20)\n",
    "print(len(date))\n",
    "print('-'*20)\n",
    "print(len(ratings))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizamos los datos para ver qué se obtiene y lo guardamos en un fichero .csv.\n",
    "\n",
    "La intención es depurar el codigo para un mejor manejo de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Subname</th>\n",
       "      <th>Description</th>\n",
       "      <th>Characteristics</th>\n",
       "      <th>Price</th>\n",
       "      <th>Reduced Price</th>\n",
       "      <th>date</th>\n",
       "      <th>User</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TOBOGANE HOT RABBIT</td>\n",
       "      <td>[el superventas de amantis ¡mejorado!]</td>\n",
       "      <td>[Vuelve nuestro vibrador de doble estimulación...</td>\n",
       "      <td>[, , Medidas: 19cm (11cm insertables) y 3,3cm/...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>39.99</td>\n",
       "      <td>[martes 22 noviembre, 2022, jueves 07 julio, 2...</td>\n",
       "      <td>[Rossi, Marina, Jennifer, Noa, Karen, Lorena, ...</td>\n",
       "      <td>[5, 5, 5, 4, 3, 5, 5, 5, 5, 5, 4, 5, 5, 3, 5, ...</td>\n",
       "      <td>[Mi primera compra. Me encantó la textura, los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIZO 2</td>\n",
       "      <td>[Dildo de suave silicona en 3 tamaños]</td>\n",
       "      <td>[En un azulejo de la cocina, en una puerta, en...</td>\n",
       "      <td>[Tanto para tus momentos de onanismo como para...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>17.99</td>\n",
       "      <td>[domingo 12 marzo, 2023, martes 10 enero, 2023...</td>\n",
       "      <td>[Iria, Barney, Sara, Aida, Lucas, antonio, Jes...</td>\n",
       "      <td>[5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[Al no tener tope viene perfecto para usar con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOXTAIL</td>\n",
       "      <td>[plug anal cola de zorro de 35cm]</td>\n",
       "      <td>[Deja volar la imaginación y saca tu lado más ...</td>\n",
       "      <td>[, , Tamaño plug S: 6.6cm Max Dia.: 2.8cm, Col...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>9.99</td>\n",
       "      <td>[jueves 26 enero, 2023, jueves 05 enero, 2023,...</td>\n",
       "      <td>[Jonatan, Irene, Ainara, Ignacio, andrea, Alic...</td>\n",
       "      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5, 4]</td>\n",
       "      <td>[Un buen plug, muy suave, aunque con el lubric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TOBOGANE</td>\n",
       "      <td>[el vibrador doble más vendido]</td>\n",
       "      <td>[Por favor, desabróchense los cinturones de se...</td>\n",
       "      <td>[, La última atracción exclusiva de amantis es...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>36.99</td>\n",
       "      <td>[lunes 06 marzo, 2023, sábado 04 marzo, 2023, ...</td>\n",
       "      <td>[Teresa, Alicia, María, Sara, Raquel, Sara, Da...</td>\n",
       "      <td>[5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, ...</td>\n",
       "      <td>[Fue el primer vibrador que compré. Me lo reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MENEO sube y baja</td>\n",
       "      <td>[placer realista con control remoto]</td>\n",
       "      <td>[Si te gusta que te metan un buen meneo, hazte...</td>\n",
       "      <td>[Ya lo ves, Meneo’s Cock puede ir contigo a do...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>44.99</td>\n",
       "      <td>[miércoles 14 diciembre, 2022, miércoles 09 no...</td>\n",
       "      <td>[Francisco, Maria, Jose Javier, Carlos, Pedro,...</td>\n",
       "      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4]</td>\n",
       "      <td>[Quería saber cuantos cm tiene la longitud que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>JUSTISSE NECK - collar + bloqueadores de muñecas</td>\n",
       "      <td>[No hay datos]</td>\n",
       "      <td>[Muy a menudo, las parejas se quejan de que la...</td>\n",
       "      <td>[\\r, Con este kit JUSTISSE NECK solo tienes un...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>23.99</td>\n",
       "      <td>[domingo 08 noviembre, 2020]</td>\n",
       "      <td>[Alejandro]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[Es un poco engorroso a la hora de ponerlo sob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>MS-VULVAN. Estimulador Total de Vulva (Cabezal...</td>\n",
       "      <td>[No hay datos]</td>\n",
       "      <td>[La orgásmica colección de accesorios para aco...</td>\n",
       "      <td>[\\r, Realizados en un agradable TPR liso y sin...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>[jueves 02 agosto, 2018, martes 17 julio, 2018...</td>\n",
       "      <td>[Inma, yulema, Cristina, Veturian, Pilar, Iván...</td>\n",
       "      <td>[5, 5, 3, 5, 5, 4, 3]</td>\n",
       "      <td>[Es uno de mis cabezales estrella, es muy como...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>AVATAR SIMON PRIDE</td>\n",
       "      <td>[Orgulloso dildo de silicona]</td>\n",
       "      <td>[¿Quién dijo que en el mundo de los dildos no ...</td>\n",
       "      <td>[, Juguete de silicona 100%, Dimensiones: 20 c...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>34.99</td>\n",
       "      <td>[martes 06 agosto, 2019]</td>\n",
       "      <td>[Javier]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[A mi chica no le gustaba la típica apariencia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>TSUNAMI FORMAS</td>\n",
       "      <td>[Fundas de silicona para TSUNAMI POWER]</td>\n",
       "      <td>[¿Has echado un vistazo a nuestro productazo l...</td>\n",
       "      <td>[, , Un dildo con anilla, porque hay veces en ...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>14.99</td>\n",
       "      <td>[miércoles 11 noviembre, 2020, miércoles 01 ju...</td>\n",
       "      <td>[Christian, Carmen, Meritxell, Esther, L04]</td>\n",
       "      <td>[5, 5, 4, 5, 5]</td>\n",
       "      <td>[Imprescindibles para disfrutar del Tsunami co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>Cabezal para masaje-sexy</td>\n",
       "      <td>[Triple Acción!]</td>\n",
       "      <td>[El Masaje-Sexy de amantis es uno de nuestros ...</td>\n",
       "      <td>[Este cabezal para masaje sexy es el complemen...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>9.99</td>\n",
       "      <td>[miércoles 25 diciembre, 2019, miércoles 30 en...</td>\n",
       "      <td>[BEATRIZ, Anna Maria, MARIBEL, Leonardo, joan,...</td>\n",
       "      <td>[5, 3, 5, 3, 5, 5, 4, 5, 3, 4, 4, 5, 4, 4, 4, 5]</td>\n",
       "      <td>[A pesar de haber leído en los comentarios que...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Name  \\\n",
       "0                                  TOBOGANE HOT RABBIT   \n",
       "1                                               LIZO 2   \n",
       "2                                              FOXTAIL   \n",
       "3                                             TOBOGANE   \n",
       "4                                    MENEO sube y baja   \n",
       "..                                                 ...   \n",
       "571   JUSTISSE NECK - collar + bloqueadores de muñecas   \n",
       "572  MS-VULVAN. Estimulador Total de Vulva (Cabezal...   \n",
       "573                                 AVATAR SIMON PRIDE   \n",
       "574                                     TSUNAMI FORMAS   \n",
       "575                           Cabezal para masaje-sexy   \n",
       "\n",
       "                                     Subname  \\\n",
       "0     [el superventas de amantis ¡mejorado!]   \n",
       "1     [Dildo de suave silicona en 3 tamaños]   \n",
       "2          [plug anal cola de zorro de 35cm]   \n",
       "3            [el vibrador doble más vendido]   \n",
       "4       [placer realista con control remoto]   \n",
       "..                                       ...   \n",
       "571                           [No hay datos]   \n",
       "572                           [No hay datos]   \n",
       "573            [Orgulloso dildo de silicona]   \n",
       "574  [Fundas de silicona para TSUNAMI POWER]   \n",
       "575                         [Triple Acción!]   \n",
       "\n",
       "                                           Description  \\\n",
       "0    [Vuelve nuestro vibrador de doble estimulación...   \n",
       "1    [En un azulejo de la cocina, en una puerta, en...   \n",
       "2    [Deja volar la imaginación y saca tu lado más ...   \n",
       "3    [Por favor, desabróchense los cinturones de se...   \n",
       "4    [Si te gusta que te metan un buen meneo, hazte...   \n",
       "..                                                 ...   \n",
       "571  [Muy a menudo, las parejas se quejan de que la...   \n",
       "572  [La orgásmica colección de accesorios para aco...   \n",
       "573  [¿Quién dijo que en el mundo de los dildos no ...   \n",
       "574  [¿Has echado un vistazo a nuestro productazo l...   \n",
       "575  [El Masaje-Sexy de amantis es uno de nuestros ...   \n",
       "\n",
       "                                       Characteristics  Price  Reduced Price  \\\n",
       "0    [, , Medidas: 19cm (11cm insertables) y 3,3cm/...  21.99          39.99   \n",
       "1    [Tanto para tus momentos de onanismo como para...  21.99          17.99   \n",
       "2    [, , Tamaño plug S: 6.6cm Max Dia.: 2.8cm, Col...  21.99           9.99   \n",
       "3    [, La última atracción exclusiva de amantis es...  21.99          36.99   \n",
       "4    [Ya lo ves, Meneo’s Cock puede ir contigo a do...  21.99          44.99   \n",
       "..                                                 ...    ...            ...   \n",
       "571  [\\r, Con este kit JUSTISSE NECK solo tienes un...  21.99          23.99   \n",
       "572  [\\r, Realizados en un agradable TPR liso y sin...  21.99           8.99   \n",
       "573  [, Juguete de silicona 100%, Dimensiones: 20 c...  21.99          34.99   \n",
       "574  [, , Un dildo con anilla, porque hay veces en ...  21.99          14.99   \n",
       "575  [Este cabezal para masaje sexy es el complemen...  21.99           9.99   \n",
       "\n",
       "                                                  date  \\\n",
       "0    [martes 22 noviembre, 2022, jueves 07 julio, 2...   \n",
       "1    [domingo 12 marzo, 2023, martes 10 enero, 2023...   \n",
       "2    [jueves 26 enero, 2023, jueves 05 enero, 2023,...   \n",
       "3    [lunes 06 marzo, 2023, sábado 04 marzo, 2023, ...   \n",
       "4    [miércoles 14 diciembre, 2022, miércoles 09 no...   \n",
       "..                                                 ...   \n",
       "571                       [domingo 08 noviembre, 2020]   \n",
       "572  [jueves 02 agosto, 2018, martes 17 julio, 2018...   \n",
       "573                           [martes 06 agosto, 2019]   \n",
       "574  [miércoles 11 noviembre, 2020, miércoles 01 ju...   \n",
       "575  [miércoles 25 diciembre, 2019, miércoles 30 en...   \n",
       "\n",
       "                                                  User  \\\n",
       "0    [Rossi, Marina, Jennifer, Noa, Karen, Lorena, ...   \n",
       "1    [Iria, Barney, Sara, Aida, Lucas, antonio, Jes...   \n",
       "2    [Jonatan, Irene, Ainara, Ignacio, andrea, Alic...   \n",
       "3    [Teresa, Alicia, María, Sara, Raquel, Sara, Da...   \n",
       "4    [Francisco, Maria, Jose Javier, Carlos, Pedro,...   \n",
       "..                                                 ...   \n",
       "571                                        [Alejandro]   \n",
       "572  [Inma, yulema, Cristina, Veturian, Pilar, Iván...   \n",
       "573                                           [Javier]   \n",
       "574        [Christian, Carmen, Meritxell, Esther, L04]   \n",
       "575  [BEATRIZ, Anna Maria, MARIBEL, Leonardo, joan,...   \n",
       "\n",
       "                                               Ratings  \\\n",
       "0    [5, 5, 5, 4, 3, 5, 5, 5, 5, 5, 4, 5, 5, 3, 5, ...   \n",
       "1    [5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "2                       [5, 5, 5, 5, 5, 5, 5, 5, 5, 4]   \n",
       "3    [5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, ...   \n",
       "4              [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4]   \n",
       "..                                                 ...   \n",
       "571                                                [5]   \n",
       "572                              [5, 5, 3, 5, 5, 4, 3]   \n",
       "573                                                [4]   \n",
       "574                                    [5, 5, 4, 5, 5]   \n",
       "575   [5, 3, 5, 3, 5, 5, 4, 5, 3, 4, 4, 5, 4, 4, 4, 5]   \n",
       "\n",
       "                                               Comment  \n",
       "0    [Mi primera compra. Me encantó la textura, los...  \n",
       "1    [Al no tener tope viene perfecto para usar con...  \n",
       "2    [Un buen plug, muy suave, aunque con el lubric...  \n",
       "3    [Fue el primer vibrador que compré. Me lo reco...  \n",
       "4    [Quería saber cuantos cm tiene la longitud que...  \n",
       "..                                                 ...  \n",
       "571  [Es un poco engorroso a la hora de ponerlo sob...  \n",
       "572  [Es uno de mis cabezales estrella, es muy como...  \n",
       "573  [A mi chica no le gustaba la típica apariencia...  \n",
       "574  [Imprescindibles para disfrutar del Tsunami co...  \n",
       "575  [A pesar de haber leído en los comentarios que...  \n",
       "\n",
       "[576 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe= pd.DataFrame({\"Name\": name,\"Subname\": subname,\"Description\": info, \"Characteristics\": charac,\"Price\":regular_price,\"Reduced Price\":new_price,\n",
    "                         \"date\":date,\"User\": user_comments,\"Ratings\": ratings,\"Comment\": comment})\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data_science\\\\Javier\\\\Repositorios\\\\Proyecto_tienda_online\\\\Data\\\\scrapped_data.csv'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=folder+'\\\\Data\\\\scrapped_data.csv'\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(file,header=True,index=False)           # Tengo que generar el path correcto\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solucionando inconvenientes de los datos obtenidos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Subname* puede generar problemas por lo eliminaremos del codigo. Lo generaremos después a partir de *Name*.\n",
    "- *Characteristics* puede generar problemas, ya que serían datos de dimensiones, duración, etc. Lo generaremos después a partir de *Description*.\n",
    "- Hay que pasar las listas a registros individuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.amantis.net/lizo-2-dildo-suave-silicona-3-tamanos/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_URLs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.amantis.net/productos-amantis/\"              # lista productos\n",
    "url_principal=\"https://www.amantis.net/\"                        # productos\n",
    "# pages= np.arange(1,5)\n",
    "pages= np.arange(1, 25)\n",
    "\n",
    "'''Listas a generar con la información de los productos'''\n",
    "lista_URLs = []\n",
    "\n",
    "''' Obtenemos las URLs de los productos para entrar luego en sus URLS y extraer la información'''\n",
    "\n",
    "for page in pages:\n",
    "    if page == 1:\n",
    "        print(\"Leyendo paginas\")\n",
    "        # print(\"Pagina\",page)\n",
    "        URL = url\n",
    "        response = requests.get(url)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        productos = soup.find_all(class_='caption')\n",
    "        for producto in productos[8:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "        \n",
    "    else:\n",
    "#        print(\"Pagina\",page)\n",
    "        URL = url+'page' + str(page)+'/'\n",
    "        response = requests.get(URL)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        productos = soup.find_all(class_='caption')\n",
    "        for producto in productos[8:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "print(\"Terminando lectura.\\nRecabando información.\")\n",
    "\n",
    "list_name=[]\n",
    "list_regular_prices=[]\n",
    "list_new_price=[]\n",
    "list_info=[]\n",
    "user_comments=[]\n",
    "comment=[]\n",
    "date=[]\n",
    "ratings=[]\n",
    "datas=[]\n",
    "diccionario_URL={}\n",
    "\n",
    "\n",
    "\n",
    "'''Extraemos la información de cada producto existente'''\n",
    "\n",
    "for URL in lista_URLs:\n",
    "    url_product=URL\n",
    "    response_product = requests.get(url_product)\n",
    "    soup_product = bs(response_product.text, 'lxml')\n",
    "    user_comments_product=[]\n",
    "    date_comments_product=[]\n",
    "    comments_product=[]\n",
    "    rating=[]\n",
    "    data=[]\n",
    "    name=[]\n",
    "    regular_prices=[]\n",
    "    new_price=[]\n",
    "    info=[]\n",
    "\n",
    "    diccionario_comments={}\n",
    "\n",
    "    titulos=soup_product.find_all(\"h1\",class_=\"h3\")\n",
    "    for titulo in titulos:\n",
    "        nombre=titulo.get_text(strip=True)\n",
    "        name.append(nombre)\n",
    "\n",
    "    description=soup_product.find(\"div\", class_=\"description\") \n",
    "    information=description.get_text().split('\\n')[1:]\n",
    "    documentation = ''.join(information)\n",
    "    info.append(documentation)\n",
    "\n",
    "\n",
    "    all_price = soup_product.find_all(\"div\", class_=\"productoPrecio pull-right tdd_precio\")                        \n",
    "    for price_container in all_price:                                                                    \n",
    "        try:\n",
    "            special_price = price_container.find(\"span\", class_=\"productSpecialPrice\")\n",
    "            if special_price:\n",
    "                item_price = float(special_price.get_text(strip=True).replace(\",\", \".\").split('€')[0])\n",
    "                new_price.append(item_price)\n",
    "                regular_price = price_container.find(\"del\").get_text(strip=True)\n",
    "                item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                regular_prices.append(item_regular_price)\n",
    "            else:\n",
    "                regular_price = price_container.find(\"span\").get_text(strip=True)\n",
    "                item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                new_price.append(item_regular_price)\n",
    "                regular_prices.append(None)\n",
    "        except:\n",
    "            new_price.append(None)\n",
    "            regular_prices.append(None)\n",
    "\n",
    "    for i, regular_price in enumerate(regular_prices):\n",
    "        if regular_price is None:\n",
    "            regular_prices[i] = new_price[i]\n",
    "\n",
    "\n",
    "    '''Vamos a obtener los datos de los comentarios de los usuarios'''\n",
    "    all_user_comments = soup_product.find_all(\"span\", class_=\"name-user\") \n",
    "    for user_comment in all_user_comments:\n",
    "        user_comments_product.append(user_comment.get_text(strip=True))\n",
    "    all_dates = soup_product.find_all(\"span\", class_=\"date\")  \n",
    "    for dates in all_dates:\n",
    "        dates_text=dates.get_text(strip=True)\n",
    "        date_comments_product.append(dates_text)\n",
    "    all_comments = soup_product.find_all(\"p\")\n",
    "    for formats in all_comments[-len(date_comments_product):]:\n",
    "        comments_product.append(formats.get_text(strip=True))\n",
    "    hearts = soup_product.find_all('div', class_= 'box-description')\n",
    "    for heart in hearts:\n",
    "        heart_rating = heart.find_all('span', class_= 'fas fa-heart')\n",
    "        num_hearts = str(len(heart_rating))\n",
    "        rating.append(num_hearts)\n",
    "    \n",
    "    # diccionario={}\n",
    "    user_comments.append(user_comments_product)\n",
    "    date.append(date_comments_product)\n",
    "    comment.append(comments_product)\n",
    "    ratings.append(rating)\n",
    "    \n",
    "    # datos = list(zip(nombre,user_comments_product,comments_product, date_comments_product, rating))\n",
    "    # for dato in datos:\n",
    "    #     nombre=dato[0]\n",
    "    #     comentarios=dato[1:]\n",
    "    #     if nombre in diccionario_comments:\n",
    "    #         diccionario_comments[nombre].append(comentarios)\n",
    "    #     else:\n",
    "    #         diccionario_comments[nombre] = [comentarios]\n",
    "    # # diccionario_URL[name] = name\n",
    "    # diccionario_URL[URL] = diccionario_comments\n",
    "    \n",
    "    \n",
    "# diccionario_URL\n",
    "\n",
    "\n",
    "dataframe= pd.DataFrame({\"Name\": name,\"Description\": info,\"Price\":regular_price,\"Reduced Price\":new_price,\n",
    "                         \"date\":date,\"User\": user_comments,\"Ratings\": ratings,\"Comment\": comment\n",
    "                        #  ,\"diccionario\":diccionario\n",
    "                        })\n",
    "dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando las sustituciones pertinentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagina 1\n",
      "Pagina 2\n",
      "Pagina 3\n",
      "Pagina 4\n",
      "Pagina 5\n",
      "Pagina 6\n",
      "Pagina 7\n",
      "Pagina 8\n",
      "Pagina 9\n",
      "Pagina 10\n",
      "Pagina 11\n",
      "Pagina 12\n",
      "Pagina 13\n",
      "Pagina 14\n",
      "Pagina 15\n",
      "Pagina 16\n",
      "Pagina 17\n",
      "Pagina 18\n",
      "Pagina 19\n",
      "Pagina 20\n",
      "Pagina 21\n",
      "Pagina 22\n",
      "Pagina 23\n",
      "Pagina 24\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.amantis.net/productos-amantis/\"              # lista productos\n",
    "url_principal=\"https://www.amantis.net/\"                        # productos\n",
    "# pages= np.arange(1,5)\n",
    "pages= np.arange(1, 25)\n",
    "\n",
    "'''Listas a generar con la información de los productos'''\n",
    "lista_URLs = []\n",
    "name=[]\n",
    "regular_prices=[]\n",
    "new_price=[]\n",
    "info=[]\n",
    "user_comments=[]\n",
    "comment=[]\n",
    "date=[]\n",
    "ratings=[]\n",
    "\n",
    "''' Obtenemos las URLs de los productos para entrar luego en sus URLS y extraer la información'''\n",
    "\n",
    "for page in pages:\n",
    "    if page == 1:\n",
    "        print(\"Pagina\",page)\n",
    "        URL = url\n",
    "        response = requests.get(url)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        productos = soup.find_all(class_='caption')\n",
    "        for producto in productos[8:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "        \n",
    "    else:\n",
    "        print(\"Pagina\",page)\n",
    "        URL = url+'page' + str(page)+'/'\n",
    "        response = requests.get(URL)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        productos = soup.find_all(class_='caption')\n",
    "        for producto in productos[8:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "\n",
    "'''Extraemos la información de cada producto existente'''\n",
    "\n",
    "for URL in lista_URLs:\n",
    "    url_product=URL\n",
    "    response_product = requests.get(url_product)\n",
    "    soup_product = bs(response_product.text, 'lxml')\n",
    "    user_comments_product=[]\n",
    "    date_comments_product=[]\n",
    "    comments_product=[]\n",
    "    rating=[]\n",
    "    \n",
    "\n",
    "    titulos=soup_product.find_all(\"h1\",class_=\"h3\")\n",
    "    for titulo in titulos:\n",
    "        nombre=titulo.get_text(strip=True)\n",
    "        name.append(nombre)\n",
    "\n",
    "    all_price = soup_product.find_all(\"div\", class_=\"productoPrecio pull-right tdd_precio\")                        \n",
    "    for price_container in all_price:                                                                    \n",
    "        try:\n",
    "            special_price = price_container.find(\"span\", class_=\"productSpecialPrice\")\n",
    "            if special_price:\n",
    "                item_price = float(special_price.get_text(strip=True).replace(\",\", \".\").split('€')[0])\n",
    "                new_price.append(item_price)\n",
    "                regular_price = price_container.find(\"del\").get_text(strip=True)\n",
    "                item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                regular_prices.append(item_regular_price)\n",
    "            else:\n",
    "                regular_price = price_container.find(\"span\").get_text(strip=True)\n",
    "                item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                new_price.append(item_regular_price)\n",
    "                regular_prices.append(None)\n",
    "        except:\n",
    "            new_price.append(None)\n",
    "            regular_prices.append(None)\n",
    "\n",
    "    description=soup_product.find(\"div\", class_=\"description\") \n",
    "    information=description.get_text().split('\\n')[1:]\n",
    "    documentation = ''.join(information)\n",
    "    info.append(documentation)\n",
    "\n",
    "\n",
    "    '''Vamos a obtener los datos de los comentarios de los usuarios'''\n",
    "\n",
    "    all_user_comments = soup_product.find_all(\"span\", class_=\"name-user\") \n",
    "    for user_comment in all_user_comments:\n",
    "        user_comments_product.append(user_comment.get_text(strip=True))\n",
    "    user_comments.append(user_comments_product)\n",
    "\n",
    "    all_dates = soup_product.find_all(\"span\", class_=\"date\")  \n",
    "    for dates in all_dates:\n",
    "        dates_text=dates.get_text(strip=True)\n",
    "        # dates=datetime.strftime(dates, '%dd/%mm/%Y')\n",
    "        date_comments_product.append(dates_text)\n",
    "        # date_object = datetime.strptime(date_comments_product)\n",
    "    date.append(date_comments_product)\n",
    "\n",
    "    all_comments = soup_product.find_all(\"p\")\n",
    "    for formats in all_comments[-len(date_comments_product):]:\n",
    "        comments_product.append(formats.get_text(strip=True))\n",
    "    comment.append(comments_product)\n",
    "\n",
    "    hearts = soup_product.find_all('div', class_= 'box-description')\n",
    "    for heart in hearts:\n",
    "        heart_rating = heart.find_all('span', class_= 'fas fa-heart')\n",
    "        num_hearts = len(heart_rating)\n",
    "        rating.append(num_hearts)\n",
    "    ratings.append(rating)\n",
    "\n",
    "\n",
    "for i, regular_price in enumerate(regular_prices):\n",
    "    if regular_price is None:\n",
    "        regular_prices[i] = new_price[i]\n",
    "\n",
    "\n",
    "# print('-'*20)\n",
    "# print(name)\n",
    "print('-'*20)\n",
    "print(len(name))\n",
    "# print('-'*20)\n",
    "# print(subname)\n",
    "# print('-'*20)\n",
    "# print(len(subname))\n",
    "print('-'*20)\n",
    "# print(regular_prices)\n",
    "# print('-'*20)\n",
    "print(len(regular_prices))\n",
    "# print('-'*20)\n",
    "# print(new_price)\n",
    "print('-'*20)\n",
    "print(len(new_price))\n",
    "print('-'*20)\n",
    "print(len(lista_URLs))\n",
    "# print('-'*20)\n",
    "# print(lista_URLs)\n",
    "print('-'*20)\n",
    "print(len(info))\n",
    "# print('-'*20)\n",
    "# # print(charac)\n",
    "# print('-'*20)\n",
    "# print(len(charac))\n",
    "# print('-'*20)\n",
    "# # print(user_comments)\n",
    "print('-'*20)\n",
    "print(len(user_comments))\n",
    "# print('-'*20)\n",
    "# # print(comment)\n",
    "print('-'*20)\n",
    "print(len(comment))\n",
    "# # print('-'*20)\n",
    "# # print(date)\n",
    "print('-'*20)\n",
    "print(len(date))\n",
    "print('-'*20)\n",
    "print(len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Reduced Price</th>\n",
       "      <th>date</th>\n",
       "      <th>User</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TOBOGANE HOT RABBIT, el superventas de amantis...</td>\n",
       "      <td>Vuelve nuestro vibrador de doble estimulación ...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>39.99</td>\n",
       "      <td>[martes 22 noviembre, 2022, jueves 07 julio, 2...</td>\n",
       "      <td>[Rossi, Marina, Jennifer, Noa, Karen, Lorena, ...</td>\n",
       "      <td>[5, 5, 5, 4, 3, 5, 5, 5, 5, 5, 4, 5, 5, 3, 5, ...</td>\n",
       "      <td>[Mi primera compra. Me encantó la textura, los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIZO 2, Dildo de suave silicona en 3 tamaños</td>\n",
       "      <td>En un azulejo de la cocina, en una puerta, en ...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>17.99</td>\n",
       "      <td>[domingo 12 marzo, 2023, martes 10 enero, 2023...</td>\n",
       "      <td>[Iria, Barney, Sara, Aida, Lucas, antonio, Jes...</td>\n",
       "      <td>[5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[Al no tener tope viene perfecto para usar con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOXTAIL, plug anal cola de zorro de 35cm</td>\n",
       "      <td>Deja volar la imaginación y saca tu lado más s...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>9.99</td>\n",
       "      <td>[jueves 26 enero, 2023, jueves 05 enero, 2023,...</td>\n",
       "      <td>[Jonatan, Irene, Ainara, Ignacio, andrea, Alic...</td>\n",
       "      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5, 4]</td>\n",
       "      <td>[Un buen plug, muy suave, aunque con el lubric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TOBOGANE, el vibrador doble más vendido</td>\n",
       "      <td>Por favor, desabróchense los cinturones de seg...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>36.99</td>\n",
       "      <td>[lunes 06 marzo, 2023, sábado 04 marzo, 2023, ...</td>\n",
       "      <td>[Teresa, Alicia, María, Sara, Raquel, Sara, Da...</td>\n",
       "      <td>[5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, ...</td>\n",
       "      <td>[Fue el primer vibrador que compré. Me lo reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MENEO sube y baja, placer realista con control...</td>\n",
       "      <td>Si te gusta que te metan un buen meneo, hazte ...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>44.99</td>\n",
       "      <td>[miércoles 14 diciembre, 2022, miércoles 09 no...</td>\n",
       "      <td>[Francisco, Maria, Jose Javier, Carlos, Pedro,...</td>\n",
       "      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4]</td>\n",
       "      <td>[Quería saber cuantos cm tiene la longitud que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>JUSTISSE NECK - collar + bloqueadores de muñecas</td>\n",
       "      <td>Muy a menudo, las parejas se quejan de que la ...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>23.99</td>\n",
       "      <td>[domingo 08 noviembre, 2020]</td>\n",
       "      <td>[Alejandro]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[Es un poco engorroso a la hora de ponerlo sob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>MS-VULVAN. Estimulador Total de Vulva (Cabezal...</td>\n",
       "      <td>La orgásmica colección de accesorios para acop...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>[jueves 02 agosto, 2018, martes 17 julio, 2018...</td>\n",
       "      <td>[Inma, yulema, Cristina, Veturian, Pilar, Iván...</td>\n",
       "      <td>[5, 5, 3, 5, 5, 4, 3]</td>\n",
       "      <td>[Es uno de mis cabezales estrella, es muy como...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>AVATAR SIMON PRIDE, Orgulloso dildo de silicona</td>\n",
       "      <td>¿Quién dijo que en el mundo de los dildos no h...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>34.99</td>\n",
       "      <td>[martes 06 agosto, 2019]</td>\n",
       "      <td>[Javier]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[A mi chica no le gustaba la típica apariencia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>TSUNAMI FORMAS, Fundas de silicona para TSUNAM...</td>\n",
       "      <td>¿Has echado un vistazo a nuestro productazo la...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>14.99</td>\n",
       "      <td>[miércoles 11 noviembre, 2020, miércoles 01 ju...</td>\n",
       "      <td>[Christian, Carmen, Meritxell, Esther, L04]</td>\n",
       "      <td>[5, 5, 4, 5, 5]</td>\n",
       "      <td>[Imprescindibles para disfrutar del Tsunami co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>Cabezal para masaje-sexy, Triple Acción!</td>\n",
       "      <td>El Masaje-Sexy de amantis es uno de nuestros m...</td>\n",
       "      <td>21.99</td>\n",
       "      <td>9.99</td>\n",
       "      <td>[miércoles 25 diciembre, 2019, miércoles 30 en...</td>\n",
       "      <td>[BEATRIZ, Anna Maria, MARIBEL, Leonardo, joan,...</td>\n",
       "      <td>[5, 3, 5, 3, 5, 5, 4, 5, 3, 4, 4, 5, 4, 4, 4, 5]</td>\n",
       "      <td>[A pesar de haber leído en los comentarios que...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Name  \\\n",
       "0    TOBOGANE HOT RABBIT, el superventas de amantis...   \n",
       "1         LIZO 2, Dildo de suave silicona en 3 tamaños   \n",
       "2             FOXTAIL, plug anal cola de zorro de 35cm   \n",
       "3              TOBOGANE, el vibrador doble más vendido   \n",
       "4    MENEO sube y baja, placer realista con control...   \n",
       "..                                                 ...   \n",
       "571   JUSTISSE NECK - collar + bloqueadores de muñecas   \n",
       "572  MS-VULVAN. Estimulador Total de Vulva (Cabezal...   \n",
       "573    AVATAR SIMON PRIDE, Orgulloso dildo de silicona   \n",
       "574  TSUNAMI FORMAS, Fundas de silicona para TSUNAM...   \n",
       "575           Cabezal para masaje-sexy, Triple Acción!   \n",
       "\n",
       "                                           Description  Price  Reduced Price  \\\n",
       "0    Vuelve nuestro vibrador de doble estimulación ...  21.99          39.99   \n",
       "1    En un azulejo de la cocina, en una puerta, en ...  21.99          17.99   \n",
       "2    Deja volar la imaginación y saca tu lado más s...  21.99           9.99   \n",
       "3    Por favor, desabróchense los cinturones de seg...  21.99          36.99   \n",
       "4    Si te gusta que te metan un buen meneo, hazte ...  21.99          44.99   \n",
       "..                                                 ...    ...            ...   \n",
       "571  Muy a menudo, las parejas se quejan de que la ...  21.99          23.99   \n",
       "572  La orgásmica colección de accesorios para acop...  21.99           8.99   \n",
       "573  ¿Quién dijo que en el mundo de los dildos no h...  21.99          34.99   \n",
       "574  ¿Has echado un vistazo a nuestro productazo la...  21.99          14.99   \n",
       "575  El Masaje-Sexy de amantis es uno de nuestros m...  21.99           9.99   \n",
       "\n",
       "                                                  date  \\\n",
       "0    [martes 22 noviembre, 2022, jueves 07 julio, 2...   \n",
       "1    [domingo 12 marzo, 2023, martes 10 enero, 2023...   \n",
       "2    [jueves 26 enero, 2023, jueves 05 enero, 2023,...   \n",
       "3    [lunes 06 marzo, 2023, sábado 04 marzo, 2023, ...   \n",
       "4    [miércoles 14 diciembre, 2022, miércoles 09 no...   \n",
       "..                                                 ...   \n",
       "571                       [domingo 08 noviembre, 2020]   \n",
       "572  [jueves 02 agosto, 2018, martes 17 julio, 2018...   \n",
       "573                           [martes 06 agosto, 2019]   \n",
       "574  [miércoles 11 noviembre, 2020, miércoles 01 ju...   \n",
       "575  [miércoles 25 diciembre, 2019, miércoles 30 en...   \n",
       "\n",
       "                                                  User  \\\n",
       "0    [Rossi, Marina, Jennifer, Noa, Karen, Lorena, ...   \n",
       "1    [Iria, Barney, Sara, Aida, Lucas, antonio, Jes...   \n",
       "2    [Jonatan, Irene, Ainara, Ignacio, andrea, Alic...   \n",
       "3    [Teresa, Alicia, María, Sara, Raquel, Sara, Da...   \n",
       "4    [Francisco, Maria, Jose Javier, Carlos, Pedro,...   \n",
       "..                                                 ...   \n",
       "571                                        [Alejandro]   \n",
       "572  [Inma, yulema, Cristina, Veturian, Pilar, Iván...   \n",
       "573                                           [Javier]   \n",
       "574        [Christian, Carmen, Meritxell, Esther, L04]   \n",
       "575  [BEATRIZ, Anna Maria, MARIBEL, Leonardo, joan,...   \n",
       "\n",
       "                                               Ratings  \\\n",
       "0    [5, 5, 5, 4, 3, 5, 5, 5, 5, 5, 4, 5, 5, 3, 5, ...   \n",
       "1    [5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "2                       [5, 5, 5, 5, 5, 5, 5, 5, 5, 4]   \n",
       "3    [5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, ...   \n",
       "4              [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4]   \n",
       "..                                                 ...   \n",
       "571                                                [5]   \n",
       "572                              [5, 5, 3, 5, 5, 4, 3]   \n",
       "573                                                [4]   \n",
       "574                                    [5, 5, 4, 5, 5]   \n",
       "575   [5, 3, 5, 3, 5, 5, 4, 5, 3, 4, 4, 5, 4, 4, 4, 5]   \n",
       "\n",
       "                                               Comment  \n",
       "0    [Mi primera compra. Me encantó la textura, los...  \n",
       "1    [Al no tener tope viene perfecto para usar con...  \n",
       "2    [Un buen plug, muy suave, aunque con el lubric...  \n",
       "3    [Fue el primer vibrador que compré. Me lo reco...  \n",
       "4    [Quería saber cuantos cm tiene la longitud que...  \n",
       "..                                                 ...  \n",
       "571  [Es un poco engorroso a la hora de ponerlo sob...  \n",
       "572  [Es uno de mis cabezales estrella, es muy como...  \n",
       "573  [A mi chica no le gustaba la típica apariencia...  \n",
       "574  [Imprescindibles para disfrutar del Tsunami co...  \n",
       "575  [A pesar de haber leído en los comentarios que...  \n",
       "\n",
       "[576 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe= pd.DataFrame({\"Name\": name,\"Description\": info,\"Price\":regular_price,\"Reduced Price\":new_price,\n",
    "                         \"date\":date,\"User\": user_comments,\"Ratings\": ratings,\n",
    "                         \"Comment\": comment})\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(file,header=True,index=False)           # Tengo que generar el path correcto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>date</th>\n",
       "      <th>User</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TOBOGANE HOT RABBIT, el superventas de amantis...</td>\n",
       "      <td>[martes 22 noviembre, 2022, jueves 07 julio, 2...</td>\n",
       "      <td>[Rossi, Marina, Jennifer, Noa, Karen, Lorena, ...</td>\n",
       "      <td>[5, 5, 5, 4, 3, 5, 5, 5, 5, 5, 4, 5, 5, 3, 5, ...</td>\n",
       "      <td>[Mi primera compra. Me encantó la textura, los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BALLENATO, tu vibrador a distancia con aleta m...</td>\n",
       "      <td>[sábado 18 febrero, 2023, miércoles 01 febrero...</td>\n",
       "      <td>[Almudena, Tomabel, andrea, Carlos, maria, Mar...</td>\n",
       "      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, ...</td>\n",
       "      <td>[Sigo temblando con este juguete, menudas vibr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FRESH GIRL, 6 Kilos y 40cm de piel real disfru...</td>\n",
       "      <td>[sábado 31 diciembre, 2022, viernes 21 octubre...</td>\n",
       "      <td>[Adrian, Daniel, victor, Guillermo, Jesús, Dav...</td>\n",
       "      <td>[4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[Este juguete es bastante bueno, muy muy place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TOBOGANE, el vibrador doble más vendido</td>\n",
       "      <td>[viernes 24 febrero, 2023, miércoles 25 enero,...</td>\n",
       "      <td>[María, Sara, Raquel, Sara, Daniel, Pilar, Dav...</td>\n",
       "      <td>[3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, ...</td>\n",
       "      <td>[El producto está muy bien, muy suave, tamaño ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MENEO sube y baja, placer realista con control...</td>\n",
       "      <td>[miércoles 14 diciembre, 2022, miércoles 09 no...</td>\n",
       "      <td>[Francisco, Maria, Jose Javier, Carlos, Pedro,...</td>\n",
       "      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4]</td>\n",
       "      <td>[Quería saber cuantos cm tiene la longitud que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>Jaula de castidad masculina de acero con BARROTES</td>\n",
       "      <td>[jueves 26 agosto, 2021, domingo 13 diciembre,...</td>\n",
       "      <td>[Diego, Ricardo, Joe, Ignasi, Miguel Angel, Sara]</td>\n",
       "      <td>[4, 5, 3, 5, 5, 5]</td>\n",
       "      <td>[Hola, la compró mi chica por sorpresa después...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>COOL CUP, Cápsula para masturbación reutilizab...</td>\n",
       "      <td>[sábado 09 abril, 2022, jueves 10 marzo, 2022,...</td>\n",
       "      <td>[Ernesto, Miguel Ángel, Miguel Ángel, Juan Fra...</td>\n",
       "      <td>[5, 5, 5, 1, 5, 5, 5, 4, 5, 4, 5, 4, 1, 4, 3, ...</td>\n",
       "      <td>[Tacto muy suave, me gusta porque coje todo el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>SAUVAGE CAT- Plug rosa con cola blanca o negra</td>\n",
       "      <td>[miércoles 30 noviembre, 2022, lunes 17 enero,...</td>\n",
       "      <td>[roberto, Miguel, David, Maria Elena, Aroa, Ju...</td>\n",
       "      <td>[5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, ...</td>\n",
       "      <td>[Suave y sexy colita de gato. El plug en color...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>Xtreme, dos huevitos vibradores a compartir......</td>\n",
       "      <td>[miércoles 21 abril, 2021, sábado 13 junio, 20...</td>\n",
       "      <td>[Ana, ruben, Marta, Joan]</td>\n",
       "      <td>[4, 5, 3, 5]</td>\n",
       "      <td>[Luces y sombras en este juguetito:rnrnComo co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>SINUO 360, Curvas para mejorar tu performance</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Parece que tu navegador está bloqueando JavaS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Name  \\\n",
       "0    TOBOGANE HOT RABBIT, el superventas de amantis...   \n",
       "1    BALLENATO, tu vibrador a distancia con aleta m...   \n",
       "2    FRESH GIRL, 6 Kilos y 40cm de piel real disfru...   \n",
       "3              TOBOGANE, el vibrador doble más vendido   \n",
       "4    MENEO sube y baja, placer realista con control...   \n",
       "..                                                 ...   \n",
       "571  Jaula de castidad masculina de acero con BARROTES   \n",
       "572  COOL CUP, Cápsula para masturbación reutilizab...   \n",
       "573     SAUVAGE CAT- Plug rosa con cola blanca o negra   \n",
       "574  Xtreme, dos huevitos vibradores a compartir......   \n",
       "575      SINUO 360, Curvas para mejorar tu performance   \n",
       "\n",
       "                                                  date  \\\n",
       "0    [martes 22 noviembre, 2022, jueves 07 julio, 2...   \n",
       "1    [sábado 18 febrero, 2023, miércoles 01 febrero...   \n",
       "2    [sábado 31 diciembre, 2022, viernes 21 octubre...   \n",
       "3    [viernes 24 febrero, 2023, miércoles 25 enero,...   \n",
       "4    [miércoles 14 diciembre, 2022, miércoles 09 no...   \n",
       "..                                                 ...   \n",
       "571  [jueves 26 agosto, 2021, domingo 13 diciembre,...   \n",
       "572  [sábado 09 abril, 2022, jueves 10 marzo, 2022,...   \n",
       "573  [miércoles 30 noviembre, 2022, lunes 17 enero,...   \n",
       "574  [miércoles 21 abril, 2021, sábado 13 junio, 20...   \n",
       "575                                                 []   \n",
       "\n",
       "                                                  User  \\\n",
       "0    [Rossi, Marina, Jennifer, Noa, Karen, Lorena, ...   \n",
       "1    [Almudena, Tomabel, andrea, Carlos, maria, Mar...   \n",
       "2    [Adrian, Daniel, victor, Guillermo, Jesús, Dav...   \n",
       "3    [María, Sara, Raquel, Sara, Daniel, Pilar, Dav...   \n",
       "4    [Francisco, Maria, Jose Javier, Carlos, Pedro,...   \n",
       "..                                                 ...   \n",
       "571  [Diego, Ricardo, Joe, Ignasi, Miguel Angel, Sara]   \n",
       "572  [Ernesto, Miguel Ángel, Miguel Ángel, Juan Fra...   \n",
       "573  [roberto, Miguel, David, Maria Elena, Aroa, Ju...   \n",
       "574                          [Ana, ruben, Marta, Joan]   \n",
       "575                                                 []   \n",
       "\n",
       "                                               Ratings  \\\n",
       "0    [5, 5, 5, 4, 3, 5, 5, 5, 5, 5, 4, 5, 5, 3, 5, ...   \n",
       "1    [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, ...   \n",
       "2    [4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "3    [3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, ...   \n",
       "4              [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4]   \n",
       "..                                                 ...   \n",
       "571                                 [4, 5, 3, 5, 5, 5]   \n",
       "572  [5, 5, 5, 1, 5, 5, 5, 4, 5, 4, 5, 4, 1, 4, 3, ...   \n",
       "573  [5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, ...   \n",
       "574                                       [4, 5, 3, 5]   \n",
       "575                                                 []   \n",
       "\n",
       "                                               Comment  \n",
       "0    [Mi primera compra. Me encantó la textura, los...  \n",
       "1    [Sigo temblando con este juguete, menudas vibr...  \n",
       "2    [Este juguete es bastante bueno, muy muy place...  \n",
       "3    [El producto está muy bien, muy suave, tamaño ...  \n",
       "4    [Quería saber cuantos cm tiene la longitud que...  \n",
       "..                                                 ...  \n",
       "571  [Hola, la compró mi chica por sorpresa después...  \n",
       "572  [Tacto muy suave, me gusta porque coje todo el...  \n",
       "573  [Suave y sexy colita de gato. El plug en color...  \n",
       "574  [Luces y sombras en este juguetito:rnrnComo co...  \n",
       "575  [Parece que tu navegador está bloqueando JavaS...  \n",
       "\n",
       "[576 rows x 5 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prueba=dataframe[['Name','date','User','Ratings','Comment']]\n",
    "prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>date</th>\n",
       "      <th>User</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TOBOGANE HOT RABBIT, el superventas de amantis...</td>\n",
       "      <td>[martes 22 noviembre, 2022, jueves 07 julio, 2...</td>\n",
       "      <td>[Rossi, Marina, Jennifer, Noa, Karen, Lorena, ...</td>\n",
       "      <td>[5, 5, 5, 4, 3, 5, 5, 5, 5, 5, 4, 5, 5, 3, 5, ...</td>\n",
       "      <td>[Mi primera compra. Me encantó la textura, los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BALLENATO, tu vibrador a distancia con aleta m...</td>\n",
       "      <td>[sábado 18 febrero, 2023, miércoles 01 febrero...</td>\n",
       "      <td>[Almudena, Tomabel, andrea, Carlos, maria, Mar...</td>\n",
       "      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, ...</td>\n",
       "      <td>[Sigo temblando con este juguete, menudas vibr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FRESH GIRL, 6 Kilos y 40cm de piel real disfru...</td>\n",
       "      <td>[sábado 31 diciembre, 2022, viernes 21 octubre...</td>\n",
       "      <td>[Adrian, Daniel, victor, Guillermo, Jesús, Dav...</td>\n",
       "      <td>[4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[Este juguete es bastante bueno, muy muy place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TOBOGANE, el vibrador doble más vendido</td>\n",
       "      <td>[viernes 24 febrero, 2023, miércoles 25 enero,...</td>\n",
       "      <td>[María, Sara, Raquel, Sara, Daniel, Pilar, Dav...</td>\n",
       "      <td>[3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, ...</td>\n",
       "      <td>[El producto está muy bien, muy suave, tamaño ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MENEO sube y baja, placer realista con control...</td>\n",
       "      <td>[miércoles 14 diciembre, 2022, miércoles 09 no...</td>\n",
       "      <td>[Francisco, Maria, Jose Javier, Carlos, Pedro,...</td>\n",
       "      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4]</td>\n",
       "      <td>[Quería saber cuantos cm tiene la longitud que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>Jaula de castidad masculina de acero con BARROTES</td>\n",
       "      <td>[jueves 26 agosto, 2021, domingo 13 diciembre,...</td>\n",
       "      <td>[Diego, Ricardo, Joe, Ignasi, Miguel Angel, Sara]</td>\n",
       "      <td>[4, 5, 3, 5, 5, 5]</td>\n",
       "      <td>[Hola, la compró mi chica por sorpresa después...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>COOL CUP, Cápsula para masturbación reutilizab...</td>\n",
       "      <td>[sábado 09 abril, 2022, jueves 10 marzo, 2022,...</td>\n",
       "      <td>[Ernesto, Miguel Ángel, Miguel Ángel, Juan Fra...</td>\n",
       "      <td>[5, 5, 5, 1, 5, 5, 5, 4, 5, 4, 5, 4, 1, 4, 3, ...</td>\n",
       "      <td>[Tacto muy suave, me gusta porque coje todo el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>SAUVAGE CAT- Plug rosa con cola blanca o negra</td>\n",
       "      <td>[miércoles 30 noviembre, 2022, lunes 17 enero,...</td>\n",
       "      <td>[roberto, Miguel, David, Maria Elena, Aroa, Ju...</td>\n",
       "      <td>[5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, ...</td>\n",
       "      <td>[Suave y sexy colita de gato. El plug en color...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>Xtreme, dos huevitos vibradores a compartir......</td>\n",
       "      <td>[miércoles 21 abril, 2021, sábado 13 junio, 20...</td>\n",
       "      <td>[Ana, ruben, Marta, Joan]</td>\n",
       "      <td>[4, 5, 3, 5]</td>\n",
       "      <td>[Luces y sombras en este juguetito:rnrnComo co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>SINUO 360, Curvas para mejorar tu performance</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Parece que tu navegador está bloqueando JavaS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Name  \\\n",
       "0    TOBOGANE HOT RABBIT, el superventas de amantis...   \n",
       "1    BALLENATO, tu vibrador a distancia con aleta m...   \n",
       "2    FRESH GIRL, 6 Kilos y 40cm de piel real disfru...   \n",
       "3              TOBOGANE, el vibrador doble más vendido   \n",
       "4    MENEO sube y baja, placer realista con control...   \n",
       "..                                                 ...   \n",
       "571  Jaula de castidad masculina de acero con BARROTES   \n",
       "572  COOL CUP, Cápsula para masturbación reutilizab...   \n",
       "573     SAUVAGE CAT- Plug rosa con cola blanca o negra   \n",
       "574  Xtreme, dos huevitos vibradores a compartir......   \n",
       "575      SINUO 360, Curvas para mejorar tu performance   \n",
       "\n",
       "                                                  date  \\\n",
       "0    [martes 22 noviembre, 2022, jueves 07 julio, 2...   \n",
       "1    [sábado 18 febrero, 2023, miércoles 01 febrero...   \n",
       "2    [sábado 31 diciembre, 2022, viernes 21 octubre...   \n",
       "3    [viernes 24 febrero, 2023, miércoles 25 enero,...   \n",
       "4    [miércoles 14 diciembre, 2022, miércoles 09 no...   \n",
       "..                                                 ...   \n",
       "571  [jueves 26 agosto, 2021, domingo 13 diciembre,...   \n",
       "572  [sábado 09 abril, 2022, jueves 10 marzo, 2022,...   \n",
       "573  [miércoles 30 noviembre, 2022, lunes 17 enero,...   \n",
       "574  [miércoles 21 abril, 2021, sábado 13 junio, 20...   \n",
       "575                                                 []   \n",
       "\n",
       "                                                  User  \\\n",
       "0    [Rossi, Marina, Jennifer, Noa, Karen, Lorena, ...   \n",
       "1    [Almudena, Tomabel, andrea, Carlos, maria, Mar...   \n",
       "2    [Adrian, Daniel, victor, Guillermo, Jesús, Dav...   \n",
       "3    [María, Sara, Raquel, Sara, Daniel, Pilar, Dav...   \n",
       "4    [Francisco, Maria, Jose Javier, Carlos, Pedro,...   \n",
       "..                                                 ...   \n",
       "571  [Diego, Ricardo, Joe, Ignasi, Miguel Angel, Sara]   \n",
       "572  [Ernesto, Miguel Ángel, Miguel Ángel, Juan Fra...   \n",
       "573  [roberto, Miguel, David, Maria Elena, Aroa, Ju...   \n",
       "574                          [Ana, ruben, Marta, Joan]   \n",
       "575                                                 []   \n",
       "\n",
       "                                               Ratings  \\\n",
       "0    [5, 5, 5, 4, 3, 5, 5, 5, 5, 5, 4, 5, 5, 3, 5, ...   \n",
       "1    [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, ...   \n",
       "2    [4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "3    [3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, ...   \n",
       "4              [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4]   \n",
       "..                                                 ...   \n",
       "571                                 [4, 5, 3, 5, 5, 5]   \n",
       "572  [5, 5, 5, 1, 5, 5, 5, 4, 5, 4, 5, 4, 1, 4, 3, ...   \n",
       "573  [5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, ...   \n",
       "574                                       [4, 5, 3, 5]   \n",
       "575                                                 []   \n",
       "\n",
       "                                               Comment  \n",
       "0    [Mi primera compra. Me encantó la textura, los...  \n",
       "1    [Sigo temblando con este juguete, menudas vibr...  \n",
       "2    [Este juguete es bastante bueno, muy muy place...  \n",
       "3    [El producto está muy bien, muy suave, tamaño ...  \n",
       "4    [Quería saber cuantos cm tiene la longitud que...  \n",
       "..                                                 ...  \n",
       "571  [Hola, la compró mi chica por sorpresa después...  \n",
       "572  [Tacto muy suave, me gusta porque coje todo el...  \n",
       "573  [Suave y sexy colita de gato. El plug en color...  \n",
       "574  [Luces y sombras en este juguetito:rnrnComo co...  \n",
       "575  [Parece que tu navegador está bloqueando JavaS...  \n",
       "\n",
       "[576 rows x 5 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.amantis.net/lizo-2-dildo-suave-silicona-3-tamanos/'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_URLs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>datos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.amantis.net/lizo-2-dildo-suave-sil...</td>\n",
       "      <td>Iria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.amantis.net/lizo-2-dildo-suave-sil...</td>\n",
       "      <td>Al no tener tope viene perfecto para usar con ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.amantis.net/lizo-2-dildo-suave-sil...</td>\n",
       "      <td>domingo 12 marzo, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.amantis.net/lizo-2-dildo-suave-sil...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.amantis.net/lizo-2-dildo-suave-sil...</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>https://www.amantis.net/lizo-2-dildo-suave-sil...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.amantis.net/lizo-2-dildo-suave-sil...</td>\n",
       "      <td>Ariadna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.amantis.net/lizo-2-dildo-suave-sil...</td>\n",
       "      <td>Tacto muy bueno y agradable, se fija muy bien ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.amantis.net/lizo-2-dildo-suave-sil...</td>\n",
       "      <td>jueves 29 octubre, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.amantis.net/lizo-2-dildo-suave-sil...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  \\\n",
       "0   https://www.amantis.net/lizo-2-dildo-suave-sil...   \n",
       "0   https://www.amantis.net/lizo-2-dildo-suave-sil...   \n",
       "0   https://www.amantis.net/lizo-2-dildo-suave-sil...   \n",
       "0   https://www.amantis.net/lizo-2-dildo-suave-sil...   \n",
       "1   https://www.amantis.net/lizo-2-dildo-suave-sil...   \n",
       "..                                                ...   \n",
       "53  https://www.amantis.net/lizo-2-dildo-suave-sil...   \n",
       "54  https://www.amantis.net/lizo-2-dildo-suave-sil...   \n",
       "54  https://www.amantis.net/lizo-2-dildo-suave-sil...   \n",
       "54  https://www.amantis.net/lizo-2-dildo-suave-sil...   \n",
       "54  https://www.amantis.net/lizo-2-dildo-suave-sil...   \n",
       "\n",
       "                                                datos  \n",
       "0                                                Iria  \n",
       "0   Al no tener tope viene perfecto para usar con ...  \n",
       "0                              domingo 12 marzo, 2023  \n",
       "0                                                   5  \n",
       "1                                              Barney  \n",
       "..                                                ...  \n",
       "53                                                  5  \n",
       "54                                            Ariadna  \n",
       "54  Tacto muy bueno y agradable, se fija muy bien ...  \n",
       "54                            jueves 29 octubre, 2020  \n",
       "54                                                  5  \n",
       "\n",
       "[220 rows x 2 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.explode('datos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Iria',\n",
       " 'Al no tener tope viene perfecto para usar con arnés o con otros juegos que requieran esta condición. La forma es sencilla pero eficaz y su tamaño pequeño es ideal para empezar en el anal. La ventosa es súper potente, osea que se puede poner en un montón de superficies sin preocuparse de que se esté cayendo continuamente',\n",
       " 'domingo 12 marzo, 2023',\n",
       " '5')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['datos'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (4) does not match length of index (55)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Data_science\\Javier\\Repositorios\\Proyecto_tienda_online\\Notebook\\Proyecto_scrap.ipynb Cell 72\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Data_science/Javier/Repositorios/Proyecto_tienda_online/Notebook/Proyecto_scrap.ipynb#Y152sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataframe[\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mdataframe\u001b[39m.\u001b[39mdatos[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data_science/Javier/Repositorios/Proyecto_tienda_online/Notebook/Proyecto_scrap.ipynb#Y152sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataframe[\u001b[39m'\u001b[39m\u001b[39mcomment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mdataframe\u001b[39m.\u001b[39mdatos[\u001b[39m1\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data_science/Javier/Repositorios/Proyecto_tienda_online/Notebook/Proyecto_scrap.ipynb#Y152sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m dataframe[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mdataframe\u001b[39m.\u001b[39mdatos[\u001b[39m2\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:3978\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3975\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3976\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3977\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[1;32m-> 3978\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:4172\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   4163\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4164\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4165\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4170\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4171\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4172\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[0;32m   4174\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   4175\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m   4176\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   4177\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   4178\u001b[0m     ):\n\u001b[0;32m   4179\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4180\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:4905\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4902\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[0;32m   4904\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4905\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[0;32m   4906\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m--> 561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (4) does not match length of index (55)"
     ]
    }
   ],
   "source": [
    "dataframe['user']=dataframe.datos[0]\n",
    "\n",
    "dataframe['comment']=dataframe.datos[1]\n",
    "dataframe['date']=dataframe.datos[2]\n",
    "dataframe['rating']=dataframe.datos[3]\n",
    "\n",
    "dataframe\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afde6861a040563f15a2ec1b440faf84809f9a7bcc3c75cfd11a60e7dd448719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
