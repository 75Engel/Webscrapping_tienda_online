{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudio sobre la tienda on-line de:\n",
    "\n",
    "## [AMANTIS](https://www.amantis.net/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Por qué este estudio?\n",
    "\n",
    "Primero, este es un trabajo de análisis de datos y de predicción sobre una tienda on-line. Creo que las personas que veamos esta documentación somos adultas y lo suficientemente maduras para asumir que el sexo es parte de nuestra vida.\n",
    "\n",
    "Por este motivo, el hacer un estudio sobre una tienda on-line erótica es lo mismo que realizarlo sobre una tienda de ropa, muebles, un supermercado,...\n",
    "\n",
    "Segundo, yo soy usuario registrado de esta tienda on-line. No solo realizo compras para mí o para mi pareja sino también para otras personas. \n",
    "Es por este motivo el que recibo periodicamente correos de ofertas de esta tienda on-line y es aquí en donde entra mi motivación personal.\n",
    "\n",
    "Los correos que envían a sus usuarios están basados en ofertas que lanzan (estas ofertas están basadas en rebajas) y son muy arbitrarias. Suelo recibir correos de ofertas de determinados productos de los que NO he realizado una compra del mismo o similares. \n",
    "\n",
    "Por este motivo, considero interesante poder realizar algún tipo de estudio sobre los productos que tengan en la tienda y poder dar una mejor experiencia al comprador."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideraciones para el proyecto\n",
    "\n",
    "Este proyecto está estructurado en 4 partes:\n",
    "\n",
    "- Obtención y tratamiento de datos de la página web, a partir de *Webscrapping* y su manipulación para poder tratarlos a través de las librerías de *pandas* y *numpy*. \n",
    "   - A su vez se realizará algún tipo de visionado de los datos obtenidos para ver diversos aspectos a tener en cuenta.\n",
    "- Generación de nuevas variables, *featuring engineering*, para poder dar una mejor visión de los productos que ofrecen en la tienda.\n",
    "    - A través de la generación de estas nuevas variables, pretendemos dar un visionario más completo sobre la relación de las diversas variables entre sí y los usuarios.\n",
    "    - Estas variables pueden ser obtenidas a través de *pandas* y *NLP*.\n",
    "- Análisis de la información obtenida y generada para ver la relación existente entre usuarios, productos, fechas...\n",
    "   - Esta información será aplicada con las librerías de *panda*, *matplotlib* y *seaborn*.\n",
    "- Generación de modelados de los datos obtenidos para establecer gustos de los usuarios y dar sugerencia de otros productos que tengan etiquetas similares.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtención y tratamiento de los datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la **URL** *'https://www.amantis.net/productos-amantis/'* vamos a realizar un recorrido por las diferentes páginas que dispone y en donde están alojadas las **URLs** de los diferentes productos que disponen.\n",
    "\n",
    "Una vez que accedemos a la dirección de cada producto, vemos que hay una serie de datos que queremos recoger de la misma:\n",
    "> Nombre.\n",
    ">\n",
    "> Subnombre.\n",
    "> \n",
    "> Precio del producto.\n",
    ">\n",
    "> Precio rebajado del producto.\n",
    ">\n",
    "> Descripción.\n",
    ">\n",
    "> Información sobre el producto (tamaño, duración de baterias, limpieza ...). Esta información la obtendremos dentro de *Descripción* primeramente y en principio no la usaremos.\n",
    ">\n",
    "> Comentarios.\n",
    "\n",
    "Dentro de comentarios disponemos de más información, que consideramos interesante:\n",
    "> Usuario.\n",
    ">\n",
    "> Rating.\n",
    ">\n",
    "> Fecha comentario.\n",
    ">\n",
    "> Cuerpo del comentario.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A través de la librería para Webscrapping *Beautiful Soap* procederemos a extraer esta información y en determinados casos a tratarla para poder trabajar con ella.\n",
    "\n",
    "Para ello cargaremos las librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagina 1\n",
      "Pagina 2\n",
      "Pagina 3\n",
      "Pagina 4\n",
      "Pagina 5\n",
      "Pagina 6\n",
      "Pagina 7\n",
      "Pagina 8\n",
      "Pagina 9\n",
      "Pagina 10\n",
      "Pagina 11\n",
      "Pagina 12\n",
      "Pagina 13\n",
      "Pagina 14\n",
      "Pagina 15\n",
      "Pagina 16\n",
      "Pagina 17\n",
      "Pagina 18\n",
      "Pagina 19\n",
      "Pagina 20\n",
      "Pagina 21\n",
      "Pagina 22\n",
      "Pagina 23\n",
      "Pagina 24\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n",
      "--------------------\n",
      "576\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.amantis.net/productos-amantis/\"              # lista productos\n",
    "url_principal=\"https://www.amantis.net/\"                        # productos\n",
    "# pages= np.arange(1,5)\n",
    "pages= np.arange(1, 25)\n",
    "\n",
    "'''Listas a generar con la información de los productos'''\n",
    "lista_URLs = []\n",
    "name=[]\n",
    "regular_prices=[]\n",
    "new_price=[]\n",
    "info=[]\n",
    "user_comments=[]\n",
    "comment=[]\n",
    "date=[]\n",
    "ratings=[]\n",
    "\n",
    "''' Obtenemos las URLs de los productos para entrar luego en sus URLS y extraer la información'''\n",
    "\n",
    "for page in pages:\n",
    "    if page == 1:\n",
    "        print(\"Leyendo paginas\")\n",
    "        URL = url\n",
    "        response = requests.get(url)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        productos = soup.find_all(class_='caption')\n",
    "        for producto in productos[8:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "        \n",
    "    else:\n",
    "        print(\"Pagina\",page)\n",
    "        URL = url+'page' + str(page)+'/'\n",
    "        response = requests.get(URL)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        productos = soup.find_all(class_='caption')\n",
    "        for producto in productos[8:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "print(\"Terminando lectura.\\nRecabando información.\")\n",
    "\n",
    "\n",
    "'''Extraemos la información de cada producto existente'''\n",
    "\n",
    "for URL in lista_URLs:\n",
    "    url_product=URL\n",
    "    response_product = requests.get(url_product)\n",
    "    soup_product = bs(response_product.text, 'lxml')\n",
    "    user_comments_product=[]\n",
    "    date_comments_product=[]\n",
    "    comments_product=[]\n",
    "    rating=[]\n",
    "    \n",
    "\n",
    "    titulos=soup_product.find_all(\"h1\",class_=\"h3\")\n",
    "    for titulo in titulos:\n",
    "        nombre=titulo.get_text(strip=True)\n",
    "        name.append(nombre)\n",
    "\n",
    "    all_price = soup_product.find_all(\"div\", class_=\"productoPrecio pull-right tdd_precio\")                        \n",
    "    for price_container in all_price:                                                                    \n",
    "        try:\n",
    "            special_price = price_container.find(\"span\", class_=\"productSpecialPrice\")\n",
    "            if special_price:\n",
    "                item_price = float(special_price.get_text(strip=True).replace(\",\", \".\").split('€')[0])\n",
    "                new_price.append(item_price)\n",
    "                regular_price = price_container.find(\"del\").get_text(strip=True)\n",
    "                item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                regular_prices.append(item_regular_price)\n",
    "            else:\n",
    "                regular_price = price_container.find(\"span\").get_text(strip=True)\n",
    "                item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                new_price.append(item_regular_price)\n",
    "                regular_prices.append(None)\n",
    "        except:\n",
    "            new_price.append(None)\n",
    "            regular_prices.append(None)\n",
    "\n",
    "    description=soup_product.find(\"div\", class_=\"description\") \n",
    "    information=description.get_text().split('\\n')[1:]\n",
    "    documentation = ''.join(information)\n",
    "    info.append(documentation)\n",
    "\n",
    "\n",
    "    '''Vamos a obtener los datos de los comentarios de los usuarios'''\n",
    "\n",
    "    all_user_comments = soup_product.find_all(\"span\", class_=\"name-user\") \n",
    "    for user_comment in all_user_comments:\n",
    "        user_comments_product.append(user_comment.get_text(strip=True))\n",
    "    user_comments.append(user_comments_product)\n",
    "\n",
    "    all_dates = soup_product.find_all(\"span\", class_=\"date\")  \n",
    "    for dates in all_dates:\n",
    "        dates_text=dates.get_text(strip=True)\n",
    "        # dates=datetime.strftime(dates, '%dd/%mm/%Y')\n",
    "        date_comments_product.append(dates_text)\n",
    "        # date_object = datetime.strptime(date_comments_product)\n",
    "    date.append(date_comments_product)\n",
    "\n",
    "    all_comments = soup_product.find_all(\"p\")\n",
    "    for formats in all_comments[-len(date_comments_product):]:\n",
    "        comments_product.append(formats.get_text(strip=True))\n",
    "    comment.append(comments_product)\n",
    "\n",
    "    hearts = soup_product.find_all('div', class_= 'box-description')\n",
    "    for heart in hearts:\n",
    "        heart_rating = heart.find_all('span', class_= 'fas fa-heart')\n",
    "        num_hearts = len(heart_rating)\n",
    "        rating.append(num_hearts)\n",
    "    ratings.append(rating)\n",
    "\n",
    "\n",
    "for i, regular_price in enumerate(regular_prices):\n",
    "    if regular_price is None:\n",
    "        regular_prices[i] = new_price[i]\n",
    "\n",
    "\n",
    "print('-'*20)\n",
    "print(len(name))\n",
    "print('-'*20)\n",
    "print(len(regular_prices))\n",
    "print('-'*20)\n",
    "print(len(new_price))\n",
    "print('-'*20)\n",
    "print(len(lista_URLs))\n",
    "print('-'*20)\n",
    "print(len(info))\n",
    "print('-'*20)\n",
    "print(len(user_comments))\n",
    "print('-'*20)\n",
    "print(len(comment))\n",
    "print('-'*20)\n",
    "print(len(date))\n",
    "print('-'*20)\n",
    "print(len(ratings))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a pasar los datos a un Dataframe para un primer visionado de qué tipo de datos disponemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>subname</th>\n",
       "      <th>regular_prices</th>\n",
       "      <th>new_price</th>\n",
       "      <th>info</th>\n",
       "      <th>user_comments</th>\n",
       "      <th>date</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TOBOGANE HOT RABBIT</td>\n",
       "      <td>[el superventas de amantis ¡mejorado!]</td>\n",
       "      <td>89.99</td>\n",
       "      <td>39.99</td>\n",
       "      <td>[Vuelve nuestro vibrador de doble estimulación...</td>\n",
       "      <td>[Rossi, Marina, Jennifer, Noa, Karen, Lorena, ...</td>\n",
       "      <td>[martes 22 noviembre, 2022, jueves 07 julio, 2...</td>\n",
       "      <td>[Mi primera compra. Me encantó la textura, los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BALLENATO</td>\n",
       "      <td>[tu vibrador a distancia con aleta móvil y sum...</td>\n",
       "      <td>99.99</td>\n",
       "      <td>43.99</td>\n",
       "      <td>[De las profundidades más húmedas llega BALLEN...</td>\n",
       "      <td>[Almudena, Tomabel, andrea, Carlos, maria, Mar...</td>\n",
       "      <td>[sábado 18 febrero, 2023, miércoles 01 febrero...</td>\n",
       "      <td>[Sigo temblando con este juguete, menudas vibr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MENEO sube y baja</td>\n",
       "      <td>[placer realista con control remoto]</td>\n",
       "      <td>99.99</td>\n",
       "      <td>44.99</td>\n",
       "      <td>[Si te gusta que te metan un buen meneo, hazte...</td>\n",
       "      <td>[Francisco, Maria, Jose Javier, Carlos, Pedro,...</td>\n",
       "      <td>[miércoles 14 diciembre, 2022, miércoles 09 no...</td>\n",
       "      <td>[Quería saber cuantos cm tiene la longitud que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SAZZIA</td>\n",
       "      <td>[masturbador hiperrealista]</td>\n",
       "      <td>59.99</td>\n",
       "      <td>29.99</td>\n",
       "      <td>[El diccionario define SACIAR como el acto de ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Parece que tu navegador está bloqueando JavaS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LIZO 2</td>\n",
       "      <td>[Dildo de suave silicona en 3 tamaños]</td>\n",
       "      <td>59.99</td>\n",
       "      <td>17.99</td>\n",
       "      <td>[En un azulejo de la cocina, en una puerta, en...</td>\n",
       "      <td>[Barney, Sara, Aida, Lucas, antonio, Jesús, El...</td>\n",
       "      <td>[martes 10 enero, 2023, sábado 10 diciembre, 2...</td>\n",
       "      <td>[Los tengo los tres, empecé por el pequeño (er...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>SAUVAGE CAT- Plug rosa con cola blanca o negra</td>\n",
       "      <td>[No hay datos]</td>\n",
       "      <td>34.99</td>\n",
       "      <td>17.99</td>\n",
       "      <td>[Ten una experiencia anal tan original y diver...</td>\n",
       "      <td>[roberto, Miguel, David, Maria Elena, Aroa, Ju...</td>\n",
       "      <td>[miércoles 30 noviembre, 2022, lunes 17 enero,...</td>\n",
       "      <td>[Suave y sexy colita de gato. El plug en color...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>Xtreme</td>\n",
       "      <td>[dos huevitos vibradores a compartir... o no (...</td>\n",
       "      <td>44.99</td>\n",
       "      <td>19.99</td>\n",
       "      <td>[Llega un nuevo juguete revolucionario como ha...</td>\n",
       "      <td>[Ana, ruben, Marta, Joan]</td>\n",
       "      <td>[miércoles 21 abril, 2021, sábado 13 junio, 20...</td>\n",
       "      <td>[Luces y sombras en este juguetito:rnrnComo co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>SINUO 360</td>\n",
       "      <td>[Curvas para mejorar tu performance]</td>\n",
       "      <td>299.99</td>\n",
       "      <td>119.99</td>\n",
       "      <td>[¿Aún usas tu almohada para mejorar tu perform...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Parece que tu navegador está bloqueando JavaS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>MAX WAND el masajeador sexual más grande y pod...</td>\n",
       "      <td>[No hay datos]</td>\n",
       "      <td>149.99</td>\n",
       "      <td>79.99</td>\n",
       "      <td>[Que te duela algo ya no es excusa para no ten...</td>\n",
       "      <td>[ruben, RUBEN, Pedro, David, Aitor, carlos, Cr...</td>\n",
       "      <td>[jueves 11 agosto, 2022, viernes 06 mayo, 2022...</td>\n",
       "      <td>[le ha gustado mucho a mi mujer , vibracion su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>MASSAJI</td>\n",
       "      <td>[Potente masajeador japonés sumergible de sili...</td>\n",
       "      <td>99.99</td>\n",
       "      <td>44.99</td>\n",
       "      <td>[¿Quieres una velada perfecta tras un largo dí...</td>\n",
       "      <td>[Sara, Jesús, Sara, Sara, Álvaro, Yenifer, Mir...</td>\n",
       "      <td>[miércoles 25 enero, 2023, lunes 09 enero, 202...</td>\n",
       "      <td>[Es un vibrador discreto y eficaz. Es uno de m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  name  \\\n",
       "0                                  TOBOGANE HOT RABBIT   \n",
       "1                                            BALLENATO   \n",
       "2                                    MENEO sube y baja   \n",
       "3                                               SAZZIA   \n",
       "4                                               LIZO 2   \n",
       "..                                                 ...   \n",
       "571     SAUVAGE CAT- Plug rosa con cola blanca o negra   \n",
       "572                                             Xtreme   \n",
       "573                                          SINUO 360   \n",
       "574  MAX WAND el masajeador sexual más grande y pod...   \n",
       "575                                            MASSAJI   \n",
       "\n",
       "                                               subname regular_prices  \\\n",
       "0               [el superventas de amantis ¡mejorado!]          89.99   \n",
       "1    [tu vibrador a distancia con aleta móvil y sum...          99.99   \n",
       "2                 [placer realista con control remoto]          99.99   \n",
       "3                          [masturbador hiperrealista]          59.99   \n",
       "4               [Dildo de suave silicona en 3 tamaños]          59.99   \n",
       "..                                                 ...            ...   \n",
       "571                                     [No hay datos]          34.99   \n",
       "572  [dos huevitos vibradores a compartir... o no (...          44.99   \n",
       "573               [Curvas para mejorar tu performance]         299.99   \n",
       "574                                     [No hay datos]         149.99   \n",
       "575  [Potente masajeador japonés sumergible de sili...          99.99   \n",
       "\n",
       "    new_price                                               info  \\\n",
       "0       39.99  [Vuelve nuestro vibrador de doble estimulación...   \n",
       "1       43.99  [De las profundidades más húmedas llega BALLEN...   \n",
       "2       44.99  [Si te gusta que te metan un buen meneo, hazte...   \n",
       "3       29.99  [El diccionario define SACIAR como el acto de ...   \n",
       "4       17.99  [En un azulejo de la cocina, en una puerta, en...   \n",
       "..        ...                                                ...   \n",
       "571     17.99  [Ten una experiencia anal tan original y diver...   \n",
       "572     19.99  [Llega un nuevo juguete revolucionario como ha...   \n",
       "573    119.99  [¿Aún usas tu almohada para mejorar tu perform...   \n",
       "574     79.99  [Que te duela algo ya no es excusa para no ten...   \n",
       "575     44.99  [¿Quieres una velada perfecta tras un largo dí...   \n",
       "\n",
       "                                         user_comments  \\\n",
       "0    [Rossi, Marina, Jennifer, Noa, Karen, Lorena, ...   \n",
       "1    [Almudena, Tomabel, andrea, Carlos, maria, Mar...   \n",
       "2    [Francisco, Maria, Jose Javier, Carlos, Pedro,...   \n",
       "3                                                   []   \n",
       "4    [Barney, Sara, Aida, Lucas, antonio, Jesús, El...   \n",
       "..                                                 ...   \n",
       "571  [roberto, Miguel, David, Maria Elena, Aroa, Ju...   \n",
       "572                          [Ana, ruben, Marta, Joan]   \n",
       "573                                                 []   \n",
       "574  [ruben, RUBEN, Pedro, David, Aitor, carlos, Cr...   \n",
       "575  [Sara, Jesús, Sara, Sara, Álvaro, Yenifer, Mir...   \n",
       "\n",
       "                                                  date  \\\n",
       "0    [martes 22 noviembre, 2022, jueves 07 julio, 2...   \n",
       "1    [sábado 18 febrero, 2023, miércoles 01 febrero...   \n",
       "2    [miércoles 14 diciembre, 2022, miércoles 09 no...   \n",
       "3                                                   []   \n",
       "4    [martes 10 enero, 2023, sábado 10 diciembre, 2...   \n",
       "..                                                 ...   \n",
       "571  [miércoles 30 noviembre, 2022, lunes 17 enero,...   \n",
       "572  [miércoles 21 abril, 2021, sábado 13 junio, 20...   \n",
       "573                                                 []   \n",
       "574  [jueves 11 agosto, 2022, viernes 06 mayo, 2022...   \n",
       "575  [miércoles 25 enero, 2023, lunes 09 enero, 202...   \n",
       "\n",
       "                                               comment  \n",
       "0    [Mi primera compra. Me encantó la textura, los...  \n",
       "1    [Sigo temblando con este juguete, menudas vibr...  \n",
       "2    [Quería saber cuantos cm tiene la longitud que...  \n",
       "3    [Parece que tu navegador está bloqueando JavaS...  \n",
       "4    [Los tengo los tres, empecé por el pequeño (er...  \n",
       "..                                                 ...  \n",
       "571  [Suave y sexy colita de gato. El plug en color...  \n",
       "572  [Luces y sombras en este juguetito:rnrnComo co...  \n",
       "573  [Parece que tu navegador está bloqueando JavaS...  \n",
       "574  [le ha gustado mucho a mi mujer , vibracion su...  \n",
       "575  [Es un vibrador discreto y eficaz. Es uno de m...  \n",
       "\n",
       "[576 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_primero=pd.DataFrame({\"Name\": name,\"Description\": info,\"Price\":regular_price,\"Reduced Price\":new_price,\n",
    "                         \"date\":date,\"User\": user_comments,\"Ratings\": ratings,\n",
    "                         \"Comment\": comment})\n",
    "df_primero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>subname</th>\n",
       "      <th>regular_prices</th>\n",
       "      <th>new_price</th>\n",
       "      <th>info</th>\n",
       "      <th>user_comments</th>\n",
       "      <th>date</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>576</td>\n",
       "      <td>576</td>\n",
       "      <td>576.00</td>\n",
       "      <td>576.00</td>\n",
       "      <td>576</td>\n",
       "      <td>576</td>\n",
       "      <td>576</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>551</td>\n",
       "      <td>357</td>\n",
       "      <td>49.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>538</td>\n",
       "      <td>438</td>\n",
       "      <td>445</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>BRILLI</td>\n",
       "      <td>[No hay datos]</td>\n",
       "      <td>19.99</td>\n",
       "      <td>9.99</td>\n",
       "      <td>[La orgásmica colección de accesorios para aco...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Parece que tu navegador está bloqueando JavaS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>187</td>\n",
       "      <td>66.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>6</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name         subname  regular_prices  new_price  \\\n",
       "count      576             576          576.00     576.00   \n",
       "unique     551             357           49.00      57.00   \n",
       "top     BRILLI  [No hay datos]           19.99       9.99   \n",
       "freq         3             187           66.00      52.00   \n",
       "\n",
       "                                                     info user_comments date  \\\n",
       "count                                                 576           576  576   \n",
       "unique                                                538           438  445   \n",
       "top     [La orgásmica colección de accesorios para aco...            []   []   \n",
       "freq                                                    6           110  110   \n",
       "\n",
       "                                                  comment  \n",
       "count                                                 576  \n",
       "unique                                                547  \n",
       "top     [Parece que tu navegador está bloqueando JavaS...  \n",
       "freq                                                    5  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_primero.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 576 entries, 0 to 575\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   name            576 non-null    object\n",
      " 1   subname         576 non-null    object\n",
      " 2   regular_prices  576 non-null    object\n",
      " 3   new_price       576 non-null    object\n",
      " 4   info            576 non-null    object\n",
      " 5   user_comments   576 non-null    object\n",
      " 6   date            576 non-null    object\n",
      " 7   comment         576 non-null    object\n",
      "dtypes: object(8)\n",
      "memory usage: 36.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_primero.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tratamiento de los datos de texto con RegEx y Librerías de NLP\n",
    "\n",
    "Una vez obtenidos los datos en bruto vamos a generar los tags de cada uno de los parámetros a considerar a través de una serie de listas.\n",
    "\n",
    "Para esto vamos a eliminar las mayúsculas y los acentos en los textos pertinentes con Regex y pasaremos a lemmatizar las palabras para poder trabajar con ellas indistitantemente.\n",
    "\n",
    "Utilizaremos las siguientes librerías para el tratamiento  de la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "nlp = spacy.load('es_core_news_lg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y las siguientes funciones para utilizar con la función *apply*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_lemas(texto):\n",
    "    doc = nlp(texto)\n",
    "    lemas = [token.lemma_ for token in doc if token.is_alpha]\n",
    "    return lemas\n",
    "\n",
    "def remove_accents(text):\n",
    "    pattern = '[áéíóúÁÉÍÓÚ]'\n",
    "    replace = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u', 'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U'}\n",
    "    return re.sub(pattern, lambda match: replace[match.group()], text)\n",
    "\n",
    "def spanish_stemmer(x):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    return \" \".join([stemmer.stem(word) for word in x.split()])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generaremos los #tags con las siguientes listas. Estos tags serán nuevas columnas en el dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juguetes=['dildo','plug','vibrador','masturbador']\n",
    "BDSM=['latex','bdsm','arnes','strap','cera','ligadura','cuerda','cuero','sumision','dominacion','latigo']\n",
    "muebles=['columpio','sillon','sillones']\n",
    "lenceria=['body','panties','arnes',]\n",
    "anal=['anal']\n",
    "masculino=['hombre','masculino']\n",
    "femenino=['mujer','femenino']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afde6861a040563f15a2ec1b440faf84809f9a7bcc3c75cfd11a60e7dd448719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
